var Ae=Object.defineProperty;var Ne=(e,a,t)=>a in e?Ae(e,a,{enumerable:!0,configurable:!0,writable:!0,value:t}):e[a]=t;var ce=(e,a,t)=>(Ne(e,typeof a!="symbol"?a+"":a,t),t);import{c as B,k as ge,l as ve,Q as z,a as re,h as M,i as Me,m as Le,e as Ve}from"./dom.56a02491.js";import{u as F,a as H,c as De}from"./IndexPage.a3388c66.js";import{r as V,c as d,h as s,a9 as A,g as j,N as L,X as $,s as Re,aa as Ye,Q as Z,ab as ee,Y as ue,R as te,w as Oe,t as ze,H as je,ac as $e,i as Ee,k as ae,ad as be,p as We,$ as Fe,d as ye,y as N,a0 as W,j as _,A as I,F as we,a2 as ke,z as ne,B as S,a5 as E,G as P,E as oe,C as O,D as ie,a3 as He}from"./index.e84e2a7b.js";import{e as Je,Q as de}from"./QBtn.e689a42e.js";import{u as xe,Q as Ge}from"./use-quasar.25dfcad3.js";import{o as Qe}from"./open-url.95d67898.js";import"./vue-i18n.runtime.ae45a2ef.js";function Se(e,a){const t=V(null),l=d(()=>e.disable===!0?null:s("span",{ref:t,class:"no-outline",tabindex:-1}));function r(n){const o=a.value;n!==void 0&&n.type.indexOf("key")===0?o!==null&&document.activeElement!==o&&o.contains(document.activeElement)===!0&&o.focus():t.value!==null&&(n===void 0||o!==null&&o.contains(n.target)===!0)&&t.value.focus()}return{refocusTargetEl:l,refocusTarget:r}}const _e={name:String};function qe(e={}){return(a,t,l)=>{a[t](s("input",{class:"hidden"+(l||""),...e.value}))}}var Ce={xs:30,sm:35,md:40,lg:50,xl:60};const Ue=s("svg",{key:"svg",class:"q-radio__bg absolute non-selectable",viewBox:"0 0 24 24"},[s("path",{d:"M12,22a10,10 0 0 1 -10,-10a10,10 0 0 1 10,-10a10,10 0 0 1 10,10a10,10 0 0 1 -10,10m0,-22a12,12 0 0 0 -12,12a12,12 0 0 0 12,12a12,12 0 0 0 12,-12a12,12 0 0 0 -12,-12"}),s("path",{class:"q-radio__check",d:"M12,6a6,6 0 0 0 -6,6a6,6 0 0 0 6,6a6,6 0 0 0 6,-6a6,6 0 0 0 -6,-6"})]);var Ze=B({name:"QRadio",props:{...F,...ge,..._e,modelValue:{required:!0},val:{required:!0},label:String,leftLabel:Boolean,checkedIcon:String,uncheckedIcon:String,color:String,keepColor:Boolean,dense:Boolean,disable:Boolean,tabindex:[String,Number]},emits:["update:modelValue"],setup(e,{slots:a,emit:t}){const{proxy:l}=j(),r=H(e,l.$q),n=ve(e,Ce),o=V(null),{refocusTargetEl:h,refocusTarget:c}=Se(e,o),m=d(()=>A(e.modelValue)===A(e.val)),i=d(()=>"q-radio cursor-pointer no-outline row inline no-wrap items-center"+(e.disable===!0?" disabled":"")+(r.value===!0?" q-radio--dark":"")+(e.dense===!0?" q-radio--dense":"")+(e.leftLabel===!0?" reverse":"")),y=d(()=>{const g=e.color!==void 0&&(e.keepColor===!0||m.value===!0)?` text-${e.color}`:"";return`q-radio__inner relative-position q-radio__inner--${m.value===!0?"truthy":"falsy"}${g}`}),p=d(()=>(m.value===!0?e.checkedIcon:e.uncheckedIcon)||null),w=d(()=>e.disable===!0?-1:e.tabindex||0),b=d(()=>{const g={type:"radio"};return e.name!==void 0&&Object.assign(g,{".checked":m.value===!0,"^checked":m.value===!0?"checked":void 0,name:e.name,value:e.val}),g}),x=qe(b);function k(g){g!==void 0&&(L(g),c(g)),e.disable!==!0&&m.value!==!0&&t("update:modelValue",e.val,g)}function D(g){(g.keyCode===13||g.keyCode===32)&&L(g)}function R(g){(g.keyCode===13||g.keyCode===32)&&k(g)}return Object.assign(l,{set:k}),()=>{const g=p.value!==null?[s("div",{key:"icon",class:"q-radio__icon-container absolute-full flex flex-center no-wrap"},[s(z,{class:"q-radio__icon",name:p.value})])]:[Ue];e.disable!==!0&&x(g,"unshift"," q-radio__native q-ma-none q-pa-none");const q=[s("div",{class:y.value,style:n.value,"aria-hidden":"true"},g)];h.value!==null&&q.push(h.value);const Y=e.label!==void 0?re(a.default,[e.label]):M(a.default);return Y!==void 0&&q.push(s("div",{class:"q-radio__label q-anchor--skip"},Y)),s("div",{ref:o,class:i.value,tabindex:w.value,role:"radio","aria-label":e.label,"aria-checked":m.value===!0?"true":"false","aria-disabled":e.disable===!0?"true":void 0,onClick:k,onKeydown:D,onKeyup:R},q)}}});const Te={...F,...ge,..._e,modelValue:{required:!0,default:null},val:{},trueValue:{default:!0},falseValue:{default:!1},indeterminateValue:{default:null},checkedIcon:String,uncheckedIcon:String,indeterminateIcon:String,toggleOrder:{type:String,validator:e=>e==="tf"||e==="ft"},toggleIndeterminate:Boolean,label:String,leftLabel:Boolean,color:String,keepColor:Boolean,dense:Boolean,disable:Boolean,tabindex:[String,Number]},Pe=["update:modelValue"];function Ie(e,a){const{props:t,slots:l,emit:r,proxy:n}=j(),{$q:o}=n,h=H(t,o),c=V(null),{refocusTargetEl:m,refocusTarget:i}=Se(t,c),y=ve(t,Ce),p=d(()=>t.val!==void 0&&Array.isArray(t.modelValue)),w=d(()=>{const f=A(t.val);return p.value===!0?t.modelValue.findIndex(u=>A(u)===f):-1}),b=d(()=>p.value===!0?w.value>-1:A(t.modelValue)===A(t.trueValue)),x=d(()=>p.value===!0?w.value===-1:A(t.modelValue)===A(t.falseValue)),k=d(()=>b.value===!1&&x.value===!1),D=d(()=>t.disable===!0?-1:t.tabindex||0),R=d(()=>`q-${e} cursor-pointer no-outline row inline no-wrap items-center`+(t.disable===!0?" disabled":"")+(h.value===!0?` q-${e}--dark`:"")+(t.dense===!0?` q-${e}--dense`:"")+(t.leftLabel===!0?" reverse":"")),g=d(()=>{const f=b.value===!0?"truthy":x.value===!0?"falsy":"indet",u=t.color!==void 0&&(t.keepColor===!0||(e==="toggle"?b.value===!0:x.value!==!0))?` text-${t.color}`:"";return`q-${e}__inner relative-position non-selectable q-${e}__inner--${f}${u}`}),q=d(()=>{const f={type:"checkbox"};return t.name!==void 0&&Object.assign(f,{".checked":b.value,"^checked":b.value===!0?"checked":void 0,name:t.name,value:p.value===!0?t.val:t.trueValue}),f}),Y=qe(q),J=d(()=>{const f={tabindex:D.value,role:e==="toggle"?"switch":"checkbox","aria-label":t.label,"aria-checked":k.value===!0?"mixed":b.value===!0?"true":"false"};return t.disable===!0&&(f["aria-disabled"]="true"),f});function C(f){f!==void 0&&(L(f),i(f)),t.disable!==!0&&r("update:modelValue",G(),f)}function G(){if(p.value===!0){if(b.value===!0){const f=t.modelValue.slice();return f.splice(w.value,1),f}return t.modelValue.concat([t.val])}if(b.value===!0){if(t.toggleOrder!=="ft"||t.toggleIndeterminate===!1)return t.falseValue}else if(x.value===!0){if(t.toggleOrder==="ft"||t.toggleIndeterminate===!1)return t.trueValue}else return t.toggleOrder!=="ft"?t.trueValue:t.falseValue;return t.indeterminateValue}function Q(f){(f.keyCode===13||f.keyCode===32)&&L(f)}function K(f){(f.keyCode===13||f.keyCode===32)&&C(f)}const X=a(b,k);return Object.assign(n,{toggle:C}),()=>{const f=X();t.disable!==!0&&Y(f,"unshift",` q-${e}__native absolute q-ma-none q-pa-none`);const u=[s("div",{class:g.value,style:y.value,"aria-hidden":"true"},f)];m.value!==null&&u.push(m.value);const v=t.label!==void 0?re(l.default,[t.label]):M(l.default);return v!==void 0&&u.push(s("div",{class:`q-${e}__label q-anchor--skip`},v)),s("div",{ref:c,class:R.value,...J.value,onClick:C,onKeydown:Q,onKeyup:K},u)}}const Ke=s("div",{key:"svg",class:"q-checkbox__bg absolute"},[s("svg",{class:"q-checkbox__svg fit absolute-full",viewBox:"0 0 24 24"},[s("path",{class:"q-checkbox__truthy",fill:"none",d:"M1.73,12.91 8.1,19.28 22.79,4.59"}),s("path",{class:"q-checkbox__indet",d:"M4,14H20V10H4"})])]);var Xe=B({name:"QCheckbox",props:Te,emits:Pe,setup(e){function a(t,l){const r=d(()=>(t.value===!0?e.checkedIcon:l.value===!0?e.indeterminateIcon:e.uncheckedIcon)||null);return()=>r.value!==null?[s("div",{key:"icon",class:"q-checkbox__icon-container absolute-full flex flex-center no-wrap"},[s(z,{class:"q-checkbox__icon",name:r.value})])]:[Ke]}return Ie("checkbox",a)}}),et=B({name:"QToggle",props:{...Te,icon:String,iconColor:String},emits:Pe,setup(e){function a(t,l){const r=d(()=>(t.value===!0?e.checkedIcon:l.value===!0?e.indeterminateIcon:e.uncheckedIcon)||e.icon),n=d(()=>t.value===!0?e.iconColor:null);return()=>[s("div",{class:"q-toggle__track"}),s("div",{class:"q-toggle__thumb absolute flex flex-center no-wrap"},r.value!==void 0?[s(z,{name:r.value,color:n.value})]:void 0)]}return Ie("toggle",a)}});const Be={radio:Ze,checkbox:Xe,toggle:et},tt=Object.keys(Be);var at=B({name:"QOptionGroup",props:{...F,modelValue:{required:!0},options:{type:Array,validator:e=>e.every(a=>"value"in a&&"label"in a)},name:String,type:{default:"radio",validator:e=>tt.includes(e)},color:String,keepColor:Boolean,dense:Boolean,size:String,leftLabel:Boolean,inline:Boolean,disable:Boolean},emits:["update:modelValue"],setup(e,{emit:a,slots:t}){const{proxy:{$q:l}}=j(),r=Array.isArray(e.modelValue);e.type==="radio"?r===!0&&console.error("q-option-group: model should not be array"):r===!1&&console.error("q-option-group: model should be array in your case");const n=H(e,l),o=d(()=>Be[e.type]),h=d(()=>"q-option-group q-gutter-x-sm"+(e.inline===!0?" q-option-group--inline":"")),c=d(()=>{const i={role:"group"};return e.type==="radio"&&(i.role="radiogroup",e.disable===!0&&(i["aria-disabled"]="true")),i});function m(i){a("update:modelValue",i)}return()=>s("div",{class:h.value,...c.value},e.options.map((i,y)=>{const p=t["label-"+y]!==void 0?()=>t["label-"+y](i):t.label!==void 0?()=>t.label(i):void 0;return s("div",[s(o.value,{modelValue:e.modelValue,val:i.value,name:i.name===void 0?e.name:i.name,disable:e.disable||i.disable,label:p===void 0?i.label:null,leftLabel:i.leftLabel===void 0?e.leftLabel:i.leftLabel,color:i.color===void 0?e.color:i.color,checkedIcon:i.checkedIcon,uncheckedIcon:i.uncheckedIcon,dark:i.dark||n.value,size:i.size===void 0?e.size:i.size,dense:e.dense,keepColor:i.keepColor===void 0?e.keepColor:i.keepColor,"onUpdate:modelValue":m},p)])}))}});const se={left:!0,right:!0,up:!0,down:!0,horizontal:!0,vertical:!0},nt=Object.keys(se);se.all=!0;function he(e){const a={};for(const t of nt)e[t]===!0&&(a[t]=!0);return Object.keys(a).length===0?se:(a.horizontal===!0?a.left=a.right=!0:a.left===!0&&a.right===!0&&(a.horizontal=!0),a.vertical===!0?a.up=a.down=!0:a.up===!0&&a.down===!0&&(a.vertical=!0),a.horizontal===!0&&a.vertical===!0&&(a.all=!0),a)}const it=["INPUT","TEXTAREA"];function me(e,a){return a.event===void 0&&e.target!==void 0&&e.target.draggable!==!0&&typeof a.handler=="function"&&it.includes(e.target.nodeName.toUpperCase())===!1&&(e.qClonedBy===void 0||e.qClonedBy.indexOf(a.uid)===-1)}function ot(e){const a=[.06,6,50];return typeof e=="string"&&e.length&&e.split(":").forEach((t,l)=>{const r=parseFloat(t);r&&(a[l]=r)}),a}var rt=Me({name:"touch-swipe",beforeMount(e,{value:a,arg:t,modifiers:l}){if(l.mouse!==!0&&$.has.touch!==!0)return;const r=l.mouseCapture===!0?"Capture":"",n={handler:a,sensitivity:ot(t),direction:he(l),noop:Re,mouseStart(o){me(o,n)&&Ye(o)&&(Z(n,"temp",[[document,"mousemove","move",`notPassive${r}`],[document,"mouseup","end","notPassiveCapture"]]),n.start(o,!0))},touchStart(o){if(me(o,n)){const h=o.target;Z(n,"temp",[[h,"touchmove","move","notPassiveCapture"],[h,"touchcancel","end","notPassiveCapture"],[h,"touchend","end","notPassiveCapture"]]),n.start(o)}},start(o,h){$.is.firefox===!0&&ee(e,!0);const c=ue(o);n.event={x:c.left,y:c.top,time:Date.now(),mouse:h===!0,dir:!1}},move(o){if(n.event===void 0)return;if(n.event.dir!==!1){L(o);return}const h=Date.now()-n.event.time;if(h===0)return;const c=ue(o),m=c.left-n.event.x,i=Math.abs(m),y=c.top-n.event.y,p=Math.abs(y);if(n.event.mouse!==!0){if(i<n.sensitivity[1]&&p<n.sensitivity[1]){n.end(o);return}}else if(window.getSelection().toString()!==""){n.end(o);return}else if(i<n.sensitivity[2]&&p<n.sensitivity[2])return;const w=i/h,b=p/h;n.direction.vertical===!0&&i<p&&i<100&&b>n.sensitivity[0]&&(n.event.dir=y<0?"up":"down"),n.direction.horizontal===!0&&i>p&&p<100&&w>n.sensitivity[0]&&(n.event.dir=m<0?"left":"right"),n.direction.up===!0&&i<p&&y<0&&i<100&&b>n.sensitivity[0]&&(n.event.dir="up"),n.direction.down===!0&&i<p&&y>0&&i<100&&b>n.sensitivity[0]&&(n.event.dir="down"),n.direction.left===!0&&i>p&&m<0&&p<100&&w>n.sensitivity[0]&&(n.event.dir="left"),n.direction.right===!0&&i>p&&m>0&&p<100&&w>n.sensitivity[0]&&(n.event.dir="right"),n.event.dir!==!1?(L(o),n.event.mouse===!0&&(document.body.classList.add("no-pointer-events--children"),document.body.classList.add("non-selectable"),De(),n.styleCleanup=x=>{n.styleCleanup=void 0,document.body.classList.remove("non-selectable");const k=()=>{document.body.classList.remove("no-pointer-events--children")};x===!0?setTimeout(k,50):k()}),n.handler({evt:o,touch:n.event.mouse!==!0,mouse:n.event.mouse,direction:n.event.dir,duration:h,distance:{x:i,y:p}})):n.end(o)},end(o){n.event!==void 0&&(te(n,"temp"),$.is.firefox===!0&&ee(e,!1),n.styleCleanup!==void 0&&n.styleCleanup(!0),o!==void 0&&n.event.dir!==!1&&L(o),n.event=void 0)}};if(e.__qtouchswipe=n,l.mouse===!0){const o=l.mouseCapture===!0||l.mousecapture===!0?"Capture":"";Z(n,"main",[[e,"mousedown","mouseStart",`passive${o}`]])}$.has.touch===!0&&Z(n,"main",[[e,"touchstart","touchStart",`passive${l.capture===!0?"Capture":""}`],[e,"touchmove","noop","notPassiveCapture"]])},updated(e,a){const t=e.__qtouchswipe;t!==void 0&&(a.oldValue!==a.value&&(typeof a.value!="function"&&t.end(),t.handler=a.value),t.direction=he(a.modifiers))},beforeUnmount(e){const a=e.__qtouchswipe;a!==void 0&&(te(a,"main"),te(a,"temp"),$.is.firefox===!0&&ee(e,!1),a.styleCleanup!==void 0&&a.styleCleanup(),delete e.__qtouchswipe)}});function st(){const e=new Map;return{getCache:function(a,t){return e[a]===void 0?e[a]=t:e[a]},getCacheWithFn:function(a,t){return e[a]===void 0?e[a]=t():e[a]}}}const lt={name:{required:!0},disable:Boolean},fe={setup(e,{slots:a}){return()=>s("div",{class:"q-panel scroll",role:"tabpanel"},M(a.default))}},ct={modelValue:{required:!0},animated:Boolean,infinite:Boolean,swipeable:Boolean,vertical:Boolean,transitionPrev:String,transitionNext:String,transitionDuration:{type:[String,Number],default:300},keepAlive:Boolean,keepAliveInclude:[String,Array,RegExp],keepAliveExclude:[String,Array,RegExp],keepAliveMax:Number},ut=["update:modelValue","beforeTransition","transition"];function dt(){const{props:e,emit:a,proxy:t}=j(),{getCacheWithFn:l}=st();let r,n;const o=V(null),h=V(null);function c(u){const v=e.vertical===!0?"up":"left";C((t.$q.lang.rtl===!0?-1:1)*(u.direction===v?1:-1))}const m=d(()=>[[rt,c,void 0,{horizontal:e.vertical!==!0,vertical:e.vertical,mouse:!0}]]),i=d(()=>e.transitionPrev||`slide-${e.vertical===!0?"down":"right"}`),y=d(()=>e.transitionNext||`slide-${e.vertical===!0?"up":"left"}`),p=d(()=>`--q-transition-duration: ${e.transitionDuration}ms`),w=d(()=>typeof e.modelValue=="string"||typeof e.modelValue=="number"?e.modelValue:String(e.modelValue)),b=d(()=>({include:e.keepAliveInclude,exclude:e.keepAliveExclude,max:e.keepAliveMax})),x=d(()=>e.keepAliveInclude!==void 0||e.keepAliveExclude!==void 0);Oe(()=>e.modelValue,(u,v)=>{const T=g(u)===!0?q(u):-1;n!==!0&&J(T===-1?0:T<q(v)?-1:1),o.value!==T&&(o.value=T,a("beforeTransition",u,v),ze(()=>{a("transition",u,v)}))});function k(){C(1)}function D(){C(-1)}function R(u){a("update:modelValue",u)}function g(u){return u!=null&&u!==""}function q(u){return r.findIndex(v=>v.props.name===u&&v.props.disable!==""&&v.props.disable!==!0)}function Y(){return r.filter(u=>u.props.disable!==""&&u.props.disable!==!0)}function J(u){const v=u!==0&&e.animated===!0&&o.value!==-1?"q-transition--"+(u===-1?i.value:y.value):null;h.value!==v&&(h.value=v)}function C(u,v=o.value){let T=v+u;for(;T>-1&&T<r.length;){const U=r[T];if(U!==void 0&&U.props.disable!==""&&U.props.disable!==!0){J(u),n=!0,a("update:modelValue",U.props.name),setTimeout(()=>{n=!1});return}T+=u}e.infinite===!0&&r.length!==0&&v!==-1&&v!==r.length&&C(u,u===-1?r.length:-1)}function G(){const u=q(e.modelValue);return o.value!==u&&(o.value=u),!0}function Q(){const u=g(e.modelValue)===!0&&G()&&r[o.value];return e.keepAlive===!0?[s($e,b.value,[s(x.value===!0?l(w.value,()=>({...fe,name:w.value})):fe,{key:w.value,style:p.value},()=>u)])]:[s("div",{class:"q-panel scroll",style:p.value,key:w.value,role:"tabpanel"},[u])]}function K(){if(r.length!==0)return e.animated===!0?[s(je,{name:h.value},Q)]:Q()}function X(u){return r=Je(M(u.default,[])).filter(v=>v.props!==null&&v.props.slot===void 0&&g(v.props.name)===!0),r.length}function f(){return r}return Object.assign(t,{next:k,previous:D,goTo:R}),{panelIndex:o,panelDirectives:m,updatePanelsList:X,updatePanelIndex:G,getPanelContent:K,getEnabledPanels:Y,getPanels:f,isValidPanelName:g,keepAliveProps:b,needsUniqueKeepAliveWrapper:x,goToPanelByOffset:C,goToPanel:R,nextPanel:k,previousPanel:D}}var ht=B({name:"QTabPanel",props:lt,setup(e,{slots:a}){return()=>s("div",{class:"q-tab-panel",role:"tabpanel"},M(a.default))}}),mt=B({name:"QTabPanels",props:{...ct,...F},emits:ut,setup(e,{slots:a}){const t=j(),l=H(e,t.proxy.$q),{updatePanelsList:r,getPanelContent:n,panelDirectives:o}=dt(),h=d(()=>"q-tab-panels q-panel-parent"+(l.value===!0?" q-tab-panels--dark q-dark":""));return()=>(r(a),Le("div",{class:h.value},n(),"pan",e.swipeable,()=>o.value))}});class le{}ce(le,"Data",[{type:"Paper",title:"Searching for Galactic H ii Regions from the LAMOST Database Based on the Multihead WDCNN Model",publicationYear:2023,publicationMonth:8,authors:["Mengxin Wang","Jingjing Wu","Bin Jiang","Yanxia Zhang"],abstract:'A H ii region is a kind of emission nebula, and more definite samples of H ii regions can help study the formation and evolution of galaxies. Hence, a systematic search for H ii regions is necessary. The Large Sky Area Multi-Object Fiber Spectroscopic Telescope (LAMOST) conducts medium-resolution spectroscopic surveys and provides abundant valuable spectra for unique and rare celestial body research. Therefore, the medium-resolution spectra of LAMOST are an ideal data source for searching for Galactic H ii regions. This study uses the LAMOST spectra to expand the current spectral sample of Galactic H ii regions through machine learning. Inspired by deep convolutional neural networks with wide first-layer kernels (WDCNN), a new spectral-screening method, multihead WDCNN, is proposed and implemented. Infrared criteria are further used for the identification of Galactic H ii region candidates. Experimental results show that the multihead WDCNN model is superior to other machine-learning methods and it can effectively extract spectral features and identify H ii regions from the massive spectral database. In the end, among all candidates, 57 H ii regions are identified and known in SIMBAD, and four objects are identified as "to be confirmed" Galactic H ii region candidates. The known H ii regions and H ii region candidates can be retrieved from the LAMOST website.',doi:"10.3847/1538-4365/acd6f9",url:"https://doi.org/10.3847/1538-4365/acd6f9",publicationJournal:"The Astrophysical Journal Supplement Series",showImgUrl:"/publications/mengxinwang_2023_8.webp"},{type:"Paper",title:"Automatic detection of cataclysmic variables from SDSS images",publicationYear:2023,publicationMonth:6,authors:["Junfeng Huang","Meixia Qu","Bin Jiang","Yanxia Zhang"],abstract:"Investigating rare and new objects have always been an important direction in astronomy. Cataclysmic variables (CVs) are ideal and natural celestial bodies for studying the accretion process of semi-detached binaries with accretion processes. However, the sample size of CVs must increase because a lager gap exists between the observational and the theoretical expanding CVs. Astronomy has entered the big data era and can provide massive images containing CV candidates. CVs as a type of faint celestial objects, are highly challenging to be identified directly from images using automatic manners. Deep learning has rapidly developed in intelligent image processing and has been widely applied in some astronomical fields with excellent detection results. YOLOX, as the latest YOLO framework, is advantageous in detecting small and dark targets. This work proposes an improved YOLOX-based framework according to the characteristics of CVs and Sloan Digital Sky Survey (SDSS) photometric images to train and verify the model to realise CV detection. We use the Convolutional Block Attention Module to increase the number of output features with the feature extraction network and adjust the feature fusion network to obtain fused features. Accordingly, the loss function is modified. Experimental results demonstrate that the improved model produces satisfactory results, with average accuracy (mean average Precision at 0.5) of 92.0%, Precision of 92.9%, Recall of 94.3%, and  F1\u2212score of 93.6% on the test set. The proposed method can efficiently achieve the identification of CVs in test samples and search for CV candidates in unlabeled images. The image data vastly outnumber the spectra in the SDSS-released data. With supplementary follow-up observations or spectra, the proposed model can help astronomers in seeking and detecting CVs in a new manner to ensure that a more extensive CV catalog can be built. The proposed model may also be applied to the detection of other kinds of celestial objects.",doi:"10.1017/pasa.2023.34",url:"https://doi.org/10.1017/pasa.2023.34",publicationJournal:"Publications of the Astronomical Society of Australia",showImgUrl:"/publications/junfenghuang_2023_6.webp"},{type:"Paper",title:"Identification of Blue Horizontal Branch Stars with Multimodal Fusion",publicationYear:2023,publicationMonth:8,authors:["Jiaqi Wei","Bin Jiang","Yanxia Zhang"],abstract:"Blue Horizontal Branch stars (BHBs) are ideal tracers to probe the global structure of the milky Way (MW), and the increased size of the BHB star sample could be helpful to accurately calculate the MW's enclosed mass and kinematics. Large survey telescopes have produced an increasing number of astronomical images and spectra. However, traditional methods of identifying BHBs are limited in dealing with the large scale of astronomical data. A fast and efficient way of identifying BHBs can provide a more significant sample for further analysis and research. Therefore, in order to fully use the various data observed and further improve the identification accuracy of BHBs, we have innovatively proposed and implemented a Bi-level attention mechanism-based Transformer multimodal fusion model, called Bi-level Attention in the Transformer with Multimodality (BATMM). The model consists of a spectrum encoder, an image encoder, and a Transformer multimodal fusion module. The Transformer enables the effective fusion of data from two modalities, namely image and spectrum, by using the proposed Bi-level attention mechanism, including cross-attention and self-attention. As a result, the information from the different modalities complements each other, thus improving the accuracy of the identification of BHBs. The experimental results show that the F1 score of the proposed BATMM is 94.78%, which is 21.77% and 2.76% higher than the image and spectral unimodality, respectively. It is therefore demonstrated that higher identification accuracy of BHBs can be achieved by means of using data from multiple modalities and employing an efficient data fusion strategy.",doi:"10.1088/1538-3873/acea43",url:"https://doi.org/10.1088/1538-3873/acea43",publicationJournal:"Publications of the Astronomical Society of the Pacific",showImgUrl:"/publications/jiaqiwei_2023_8.webp"},{type:"Paper",title:"LoyalDE: Improving the performance of Graph Neural Networks with loyal node discovery and emphasis",publicationYear:2023,publicationMonth:1,authors:["Haotong Wei","Yinlin Zhu","Xunkai Li","Bin Jiang"],abstract:"Recent years have witnessed an increasing focus on graph-based semi-supervised learning with Graph Neural Networks (GNNs). Despite existing GNNs having achieved remarkable accuracy, research on the quality of graph supervision information has inadvertently been ignored. In fact, there are significant differences in the quality of supervision information provided by different labeled nodes, and treating supervision information with different qualities equally may lead to sub-optimal performance of GNNs. We refer to this as the graph supervision loyalty problem, which is a new perspective for improving the performance of GNNs. In this paper, we devise FT-Score to quantify node loyalty by considering both the local feature similarity and the local topology similarity, and nodes with higher loyalty are more likely to provide higher-quality supervision. Based on this, we propose LoyalDE (Loyal Node Discovery and Emphasis), a model-agnostic hot-plugging training strategy, which can discover potential nodes with high loyalty to expand the training set, and then emphasize nodes with high loyalty during model training to improve performance. Experiments demonstrate that the graph supervision loyalty problem will fail most existing GNNs. In contrast, LoyalDE brings about at most 9.1% performance improvement to vanilla GNNs and consistently outperforms several state-of-the-art training strategies for semi-supervised node classification.",doi:"10.1016/j.neunet.2023.05.023",url:"https://doi.org/10.1016/j.neunet.2023.05.023",publicationJournal:"Neural Networks",showImgUrl:"/publications/haotongwei_2023_1.webp"},{type:"Paper",title:"Deep Multimodal Networks for M-type Star Classification with Paired Spectrum and Photometric Image",publicationYear:2023,publicationMonth:5,authors:["Jialin Gao","Jianyu Chen","Jiaqi Wei","Bin Jiang"],abstract:"Traditional stellar classification methods include spectral and photometric classification separately. Although satisfactory results can be achieved, the accuracy could be improved. In this paper, we pioneer a novel approach to deeply fuse the spectra and photometric images of the sources in an advanced multimodal network to enhance the model's discriminatory ability. We use Transformer as the fusion module and apply a spectrum\u2013image contrastive loss function to enhance the consistency of the spectrum and photometric image of the same source in two different feature spaces. We perform M-type stellar subtype classification on two data sets with high and low signal-to-noise ratio (S/N) spectra and corresponding photometric images, and the F1-score achieves 95.65% and 90.84%, respectively. In our experiments, we prove that our model effectively utilizes the information from photometric images and is more accurate than advanced spectrum and photometric image classifiers. Our contributions can be summarized as follows: (1) We propose an innovative idea for stellar classification that allows the model to simultaneously consider information from spectra and photometric images. (2) We discover the challenge of fusing low-S/N spectra and photometric images in the Transformer and provide a solution. (3) The effectiveness of Transformer for spectral classification is discussed for the first time and will inspire more Transformer-based spectral classification models.",doi:"10.1088/1538-3873/acc7ca",url:"https://doi.org/10.1088/1538-3873/acc7ca",publicationJournal:"Publications of the Astronomical Society of the Pacific",showImgUrl:"/publications/jialingao_2023_5.webp"},{type:"Paper",title:"LS-GAN: Iterative Language-based Image Manipulation via Long and Short Term Consistency Reasoning",publicationYear:2022,publicationMonth:10,authors:["Gaoxiang Cong","Liang Li","Zhenhuan Liu","Yunbin Tu","Weijun Qin","Shenyuan Zhang","Chengang Yan","Wenyu Wang","Bin Jiang"],abstract:"Iterative language-based image manipulation aims to edit images step by step according to user's linguistic instructions. The existing methods mostly focus on aligning the attributes and appearance of new-added visual elements with current instruction. However, they fail to maintain consistency between instructions and images as iterative rounds increase. To address this issue, we propose a novel Long and Short term consistency reasoning Generative Adversarial Network (LS-GAN), which enhances the awareness of previous objects with current instruction and better maintains the consistency with the user's intent under the continuous iterations. Specifically, we first design a Context-aware Phrase Encoder (CPE) to learn the user's intention by extracting different phrase-level information about the instruction. Further, we introduce a Long and Short term Consistency Reasoning (LSCR) mechanism. The long-term reasoning improves the model on semantic understanding and positional reasoning, while short-term reasoning ensures the ability to construct visual scenes based on linguistic instructions. Extensive results show that LS-GAN improves the generation quality in terms of both object identity and position, and achieves the state-of-the-art performance on two public datasets.",doi:"10.1145/3503161.3548206",url:"https://doi.org/10.1145/3503161.3548206",publicationJournal:"Proceedings of the 29th ACM International Conference on Multimedia",showImgUrl:"/publications/gaoxiangcong_2022_10.webp"},{type:"Paper",title:"SEL-RefineMask: A Seal Segmentation and Recognition Neural Network with SEL-FPN",publicationYear:2022,publicationMonth:6,authors:["Zedong Dun","Jianyu Chen","Meixia Qu","Bin Jiang"],abstract:"Digging historical and cultural information from seals in ancient books is of great significance. However, ancient Chinese seal samples are scarce and carving methods are diverse, and traditional digital image processing methods based on greyscale have difficulty achieving superior segmentation and recognition performance. Recently, some deep learning algorithms have been proposed to address this problem; however, current neural networks are difficult to train owing to the lack of datasets. To solve the afore-mentioned problems, we proposed an SEL-RefineMask which combines selector of feature pyramid network (SEL-FPN) with RefineMask to segment and recognize seals. We designed an SEL-FPN to intelligently select a specific layer which represents different scales in the FPN and reduces the number of anchor frames. We performed experiments on some instance segmentation networks as the baseline method, and the top-1 segmentation result of 64.93% is 5.73% higher than that of humans. The top-1 result of the SEL-RefineMask network reached 67.96% which surpassed the baseline results. After segmentation, a vision transformer was used to recognize the segmentation output, and the accuracy reached 91%. Furthermore, a dataset of seals in ancient Chinese books (SACB) for segmentation and small seal font (SSF) for recognition were established which are publicly available on the website. The proposed method can be used to segment and recognize seals in ancient Chinese books.",doi:"10.3745/JIPS.02.0174",url:"https://doi.org/10.3745/JIPS.02.0174",publicationJournal:"Journal of Information Processing Systems",showImgUrl:"/publications/zedongdun_2022_6.webp"},{type:"Paper",title:"Automatic Detection and Classification of Radio Galaxy Images by Deep Learning",publicationYear:2022,publicationMonth:6,authors:["Zhen Zhang","Bin Jiang","Yanxia Zhang"],abstract:"Surveys conducted by radio astronomy observatories, such as SKA, MeerKAT, Very Large Array, and ASKAP, have generated massive astronomical images containing radio galaxies (RGs). This generation of massive RG images has imposed strict requirements on the detection and classification of RGs and makes manual classification and detection increasingly difficult, even impossible. Rapid classification and detection of images of different types of RGs help astronomers make full use of the observed astronomical image data for further processing and analysis. The classification of FRI and FRII is relatively easy, and there are more studies and literature on them at present, but FR0 and FRI are similar, so it is difficult to distinguish them. It poses a greater challenge to image processing. At present, deep learning has made breakthrough progress in the field of image analysis and processing and has preliminary applications in astronomical data processing. Compared with classification algorithms that can only classify galaxies, object detection algorithms that can locate and classify RGs simultaneously are preferred. In target detection algorithms, YOLOv5 has outstanding advantages in the classification and positioning of small targets. Therefore, we propose a deep-learning method based on an improved YOLOv5 object detection model that makes full use of multisource data, combining FIRST radio with SDSS optical image data, and realizes the automatic detection of FR0, FRI, and FRII RGs. The innovation of our work is that on the basis of the original YOLOv5 object detection model, we introduce the SE Net attention mechanism, increase the number of preset anchors, adjust the network structure of the feature pyramid, and modify the network structure, thereby allowing our model to demonstrate galaxy classification and position detection effects. Our improved model produces satisfactory results, as evidenced by experiments. Overall, the mean average precision (mAP@0.5) of our improved model on the test set reaches 89.4%, which can determine the position (R.A. and decl.) and automatically detect and classify FR0s, FRIs, and FRIIs. Our work contributes to astronomy because it allows astronomers to locate FR0, FRI, and FRII galaxies in a relatively short time and can be further combined with other astronomically generated data to study the properties of these galaxies. The target detection model can also help astronomers find FR0s, FRIs, and FRIIs in future surveys and build a large-scale star RG catalog. Moreover, our work is also useful for the detection of other types of galaxies.",doi:"10.1088/1538-3873/ac67b1",url:"https://doi.org/10.1088/1538-3873/ac67b1",publicationJournal:"Publications of the Astronomical Society of the Pacific",showImgUrl:"/publications/zhenzhang_2022_6.webp"},{type:"Paper",title:"A Scalable Deep Network for Graph Clustering via Personalized PageRank",publicationYear:2022,publicationMonth:5,authors:["Yulin Zhao","Xunkai Li","Yinlin Zhu","Jin Li","Shuo Wang","Bin Jiang"],abstract:"Recently, many models based on the combination of graph convolutional networks and deep learning have attracted extensive attention for their superior performance in graph clustering tasks. However, the existing models have the following limitations: (1) Existing models are limited by the calculation method of graph convolution, and their computational cost will increase exponentially as the graph scale grows. (2) Stacking too many convolutional layers causes the over-smoothing issue and neglects the local graph structure. (3) Expanding the range of the neighborhood and the model depth together is difficult due to the orthogonal relationship between them. Inspired by personalized pagerank and auto-encoder, we conduct the node-wise graph clustering task in the undirected simple graph as the research direction and propose a Scalable Deep Network (SDN) for graph clustering via personalized pagerank. Specifically, we utilize the combination of multi-layer perceptrons and linear propagation layer based on personalized pagerank as the backbone network (i.e., the Quasi-GNN module) and employ a DNN module for auto-encoder to learn different dimensions embeddings. After that, SDN combines the two embeddings correspondingly; then, it utilizes a dual self-supervised module to constrain the training of the embedding and clustering process. Our proposed Quasi-GNN module reduces the computational costs of traditional GNN models in a decoupled approach and solves the orthogonal relationship between the model depth and the neighborhood range. Meanwhile, it also alleviates the degraded clustering effect caused by the over-smoothing issue. We conducted experiments on five widely used graph datasets. The experimental results demonstrate that our model achieves state-of-the-art performance.",doi:"10.3390/app12115502",url:"https://doi.org/10.3390/app12115502",publicationJournal:"Applied Sciences",showImgUrl:"/publications/yulin_zhao_2022_5.webp"},{type:"Paper",title:"TribranchU-Net: A Size-Sensitive Network for Orbital Tumor Segmentation",publicationYear:2023,publicationMonth:4,authors:["Yuchen He","Wenyu Wang","Zhiming Cheng","Wei Qi","Zhigang Duan","Kai jin","Juan Ye","Shuai Wang"],abstract:"The shape and location of the orbital tumor are precious for tumor evaluation, diagnosis, and treatment, so it is essential to accurately segment the orbital tumor to assist clinicians in making reasonable decisions. Nowadays, encoder-decoder-based convolutional neural networks have been widely used in medical image segmentation tasks. However, the commonly used single tensor flow architecture cannot achieve satisfactory performance for segmentation with significant size variations. In this paper, we propose a size-sensitive deep network named TriBranchU-Net, which consists of a Small Branch with fewer down-sampling layers to enhance the feature learning of small regions and a Large Branch with atrous convolutional residual connections to enhance the feature learning of large regions. Then a multi-branch fusion module is designed to fuse the learned knowledge from different branches for the final segmentation. We built a large dataset for evaluation, including 602 CT images from 64 patients with orbital tumors. The experimental results show that our method can achieve superior performance in different sizes. Moreover, additional experimental results on the CVC-ClinicDB dataset further demonstrate that our method can better handle the size variation in segmentation.",doi:"10.1109/ISBI53787.2023.10230640",url:"https://doi.org/10.1109/ISBI53787.2023.10230640",publicationJournal:"2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI)",showImgUrl:"/publications/yuchen_he_2023_4.webp"},{type:"Paper",title:"AstroYOLO: A hybrid CNN\u2013Transformer deep-learning object-detection model for blue horizontal-branch stars",publicationYear:2023,publicationMonth:10,authors:["Yuchen He","Jingjing Wu","Wenyu Wang","Bin Jiang","Yanxia Zhang"],abstract:"Blue horizontal-branch stars (BHBs) are ideal tracers for studying the Milky Way (MW) due to their bright and nearly constant magnitude. However, an incomplete screen of BHBs from a survey would result in bias of estimation of the structure or mass of the MW. With surveys of large sky telescopes like the Sloan Digital Sky Survey (SDSS), it is possible to obtain a complete sample. Thus, detecting BHBs from massive photometric images quickly and effectually is necessary. The current acquisition methods of BHBs are mainly based on manual or semi-automatic modes. Therefore, novel approaches are required to replace manual or traditional machine-learning detection. The mainstream deep-learning-based object-detection methods are often vanilla convolutional neural networks whose ability to extract global features is limited by the receptive field of the convolution operator. Recently, a new Transformer-based method has benefited from the global receptive field advantage brought by the self-attention mechanism, exceeded the vanilla convolution model in many tasks, and achieved excellent results. Therefore, this paper proposes a hybrid convolution and Transformer model called AstroYOLO to take advantage of the convolution in local feature representation and Transformer\u2019s easier discovery of long-distance feature dependences. We conduct a comparative experiment on the 4799 SDSS DR16 photometric image dataset. The experimental results show that our model achieves 99.25% AP@50, 93.79% AP@75, and 64.45% AP@95 on the test dataset, outperforming the YOLOv3 and YOLOv4 object-detection models. In addition, we test on larger cutout images based on the same resolution. Our model can reach 99.02% AP@50, 92.00% AP@75, and 61.96% AP@95 respectively, still better than YOLOv3 and YOLOv4. These results also suggest that an appropriate size for cutout images is necessary for the performance and computation of object detection. Compared with the previous models, our model has achieved satisfactory object-detection results and can effectively improve the accuracy of BHB detection.",doi:"10.1093/pasj/psad071",url:"https://doi.org/10.1093/pasj/psad071",publicationJournal:"Publications of the Astronomical Society of Japan",showImgUrl:"/publications/yuchen_he_2023_10.webp"}]);const ft=["top","middle","bottom"];var pe=B({name:"QBadge",props:{color:String,textColor:String,floating:Boolean,transparent:Boolean,multiLine:Boolean,outline:Boolean,rounded:Boolean,label:[Number,String],align:{type:String,validator:e=>ft.includes(e)}},setup(e,{slots:a}){const t=d(()=>e.align!==void 0?{verticalAlign:e.align}:null),l=d(()=>{const r=e.outline===!0&&e.color||e.textColor;return`q-badge flex inline items-center no-wrap q-badge--${e.multiLine===!0?"multi":"single"}-line`+(e.outline===!0?" q-badge--outline":e.color!==void 0?` bg-${e.color}`:"")+(r!==void 0?` text-${r}`:"")+(e.floating===!0?" q-badge--floating":"")+(e.rounded===!0?" q-badge--rounded":"")+(e.transparent===!0?" q-badge--transparent":"")});return()=>s("div",{class:l.value,style:t.value,role:"status","aria-label":e.label},re(a.default,e.label!==void 0?[e.label]:[]))}}),pt=B({name:"QTimelineEntry",props:{heading:Boolean,tag:{type:String,default:"h3"},side:{type:String,default:"right",validator:e=>["left","right"].includes(e)},icon:String,avatar:String,color:String,title:String,subtitle:String,body:String},setup(e,{slots:a}){const t=Ee(be,ae);if(t===ae)return console.error("QTimelineEntry needs to be child of QTimeline"),ae;const l=d(()=>`q-timeline__entry q-timeline__entry--${e.side}`+(e.icon!==void 0||e.avatar!==void 0?" q-timeline__entry--icon":"")),r=d(()=>`q-timeline__dot text-${e.color||t.color}`),n=d(()=>t.layout==="comfortable"&&t.side==="left");return()=>{const o=Ve(a.default,[]);if(e.body!==void 0&&o.unshift(e.body),e.heading===!0){const m=[s("div"),s("div"),s(e.tag,{class:"q-timeline__heading-title"},o)];return s("div",{class:"q-timeline__heading"},n.value===!0?m.reverse():m)}let h;e.icon!==void 0?h=[s(z,{class:"row items-center justify-center",name:e.icon})]:e.avatar!==void 0&&(h=[s("img",{class:"q-timeline__dot-img",src:e.avatar})]);const c=[s("div",{class:"q-timeline__subtitle"},[s("span",{},M(a.subtitle,[e.subtitle]))]),s("div",{class:r.value},h),s("div",{class:"q-timeline__content"},[s("h6",{class:"q-timeline__title"},M(a.title,[e.title]))].concat(o))];return s("li",{class:l.value},n.value===!0?c.reverse():c)}}}),gt=B({name:"QTimeline",props:{...F,color:{type:String,default:"primary"},side:{type:String,default:"right",validator:e=>["left","right"].includes(e)},layout:{type:String,default:"dense",validator:e=>["dense","comfortable","loose"].includes(e)}},setup(e,{slots:a}){const t=j(),l=H(e,t.proxy.$q);We(be,e);const r=d(()=>`q-timeline q-timeline--${e.layout} q-timeline--${e.layout}--${e.side}`+(l.value===!0?" q-timeline--dark":""));return()=>s("ul",{class:r.value},M(a.default))}});const vt={class:"publication-page-panel-wrap bg-primary column flex-center full-width"},bt={class:"row justify-start items-center"},yt={key:0,class:"col-4 q-pr-md"},wt={class:"text-subtitle1 text-bold"},kt={class:"text-subtitle2"},xt={class:"abstract-p text-body1 q-my-md",style:{"text-align":"justify"}},St={class:"external-link-btns row flex-center"},_t=ye({__name:"PublicationPagePanel",props:{currentYear:{type:Number,required:!0}},setup(e){const a=e,t=V(),l=xe(),r=l.platform.is.mobile;function n(){t.value=le.Data.filter(c=>c.publicationYear===a.currentYear).sort((c,m)=>c.publicationYear===m.publicationYear?m.publicationMonth-c.publicationMonth:m.publicationYear-c.publicationYear)}function o(c){const m=new Date;return m.setMonth(c-1),m.toLocaleString(l.lang.getLocale(),{month:"long"})}function h(c){return c.authors.length===1?c.authors[0]:c.authors.length===2?c.authors.join(" and "):c.authors.slice(0,-1).join(", ")+" and "+c.authors.slice(-1)}return n(),(c,m)=>(N(),W("div",vt,[_(gt,{color:"accent"},{default:I(()=>[(N(!0),W(we,null,ke(t.value,i=>(N(),ne(pt,{key:i.doi,title:i.title},{subtitle:I(()=>[S("div",bt,[E(P(`${o(i.publicationMonth)}, ${i.publicationYear}`)+" ",1),_(pe,{class:"q-ml-md",color:"accent",outline:""},{default:I(()=>[E(P(i.type),1)]),_:2},1024)])]),default:I(()=>[S("div",{class:oe([O(r)?"":"q-pa-md","publication-info-card row justify-center items-center"])},[i.showImgUrl&&!O(r)?(N(),W("div",yt,[_(Ge,{class:"page-img shadow-2",src:i.showImgUrl,style:{height:"15rem","border-radius":"19px"},ratio:1,fit:"cover"},null,8,["src"])])):ie("",!0),S("div",{class:oe([O(r)?"col-12":"col-8","column justify-start items-start"])},[i.doi?(N(),ne(pe,{key:0,color:"accent",class:"cursor-pointer",onClick:y=>O(Qe)(`https://doi.org/${i.doi}`)},{default:I(()=>[E(" doi: "+P(i.doi),1)]),_:2},1032,["onClick"])):ie("",!0),S("span",wt,P(i.publicationJournal),1),S("span",kt,P(h(i)),1),S("p",xt,P(i.abstract),1),S("div",St,[i.codeUrl?(N(),ne(de,{key:0,class:"q-mr-md",href:i.codeUrl,target:"_blank",outline:"","no-caps":""},{default:I(()=>[_(z,{name:"fa-brands fa-github",class:"q-mr-sm",size:"xs"}),E(" "+P(c.$t("publicationPageSourceCode")),1)]),_:2},1032,["href"])):ie("",!0),_(de,{href:i.url,target:"_blank",outline:"","no-caps":""},{default:I(()=>[_(z,{name:"fas fa-external-link-alt",class:"q-mr-sm",size:"xs"}),E(" "+P(c.$t("publicationPageReadMoreBtn")),1)]),_:2},1032,["href"])])],2)],2)]),_:2},1032,["title"]))),128))]),_:1})]))}});var qt=Fe(_t,[["__scopeId","data-v-7460883e"]]);const Ct={class:"publication-page-title"},Tt={class:"publication-page-tabs q-mt-md"},Dt=ye({__name:"IndexPagePublications",setup(e){const t=xe().platform.is.mobile,l=V(new Date().getFullYear()),r=[];function n(){le.Data.forEach(o=>{r.some(h=>h.value===o.publicationYear)||r.push({label:o.publicationYear.toString(),value:o.publicationYear})})}return n(),(o,h)=>(N(),W("div",{class:oe(["index-page-publications-wrap bg-transparent column flex-center full-width",O(t)?"q-pa-md":"q-pa-xl"])},[S("div",Ct,[S("span",{class:"text-accent text-bold",style:He({fontSize:O(t)?"5vw":"2vw"})},P(o.$t("publicationPageTitle")),5)]),S("div",Tt,[_(at,{color:"accent",modelValue:l.value,"onUpdate:modelValue":h[0]||(h[0]=c=>l.value=c),options:r,inline:""},null,8,["modelValue"])]),_(mt,{modelValue:l.value,"onUpdate:modelValue":h[1]||(h[1]=c=>l.value=c),class:"bg-primary",animated:""},{default:I(()=>[(N(),W(we,null,ke(r,c=>_(ht,{key:c.value,name:c.value},{default:I(()=>[_(qt,{"current-year":c.value},null,8,["current-year"])]),_:2},1032,["name"])),64))]),_:1},8,["modelValue"])],2))}});export{Dt as default};
