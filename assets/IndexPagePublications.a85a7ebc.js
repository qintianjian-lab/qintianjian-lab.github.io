var We=Object.defineProperty;var Fe=(e,a,t)=>a in e?We(e,a,{enumerable:!0,configurable:!0,writable:!0,value:t}):e[a]=t;var pe=(e,a,t)=>(Fe(e,typeof a!="symbol"?a+"":a,t),t);import{c as q,k as qe,l as Ie,Q as H,a as me,h as M,i as Je,m as Ge,e as Qe}from"./dom.9592ddc6.js";import{u as F,a as J,c as Ue,b as Ze,d as Ke,e as ve,f as Xe,Q as et,g as oe}from"./IndexPage.4d7256cc.js";import{r as z,c,h as s,a9 as D,g as Y,N as O,X,s as tt,aa as at,Q as ne,ab as re,Y as be,R as se,w as ie,t as nt,H as Te,ac as it,q as Be,ad as ot,K as rt,ae as st,i as lt,k as le,af as Pe,p as ut,$ as ct,d as Le,y as R,a0 as te,j as A,A as L,F as Ae,a2 as Ne,z as ue,B as I,a5 as ee,G as N,C as W,E as he,D as ce,a3 as dt}from"./index.78ffc5eb.js";import{e as ht,u as Me,Q as ye}from"./QBtn.382e5024.js";import{u as Ee,Q as mt}from"./use-quasar.406134cb.js";import{o as ft}from"./open-url.d07474d0.js";import"./vue-i18n.runtime.fc6620f2.js";function Ve(e,a){const t=z(null),l=c(()=>e.disable===!0?null:s("span",{ref:t,class:"no-outline",tabindex:-1}));function r(n){const i=a.value;n!==void 0&&n.type.indexOf("key")===0?i!==null&&document.activeElement!==i&&i.contains(document.activeElement)===!0&&i.focus():t.value!==null&&(n===void 0||i!==null&&i.contains(n.target)===!0)&&t.value.focus()}return{refocusTargetEl:l,refocusTarget:r}}const ze={name:String};function De(e={}){return(a,t,l)=>{a[t](s("input",{class:"hidden"+(l||""),...e.value}))}}var Re={xs:30,sm:35,md:40,lg:50,xl:60};const gt=s("svg",{key:"svg",class:"q-radio__bg absolute non-selectable",viewBox:"0 0 24 24"},[s("path",{d:"M12,22a10,10 0 0 1 -10,-10a10,10 0 0 1 10,-10a10,10 0 0 1 10,10a10,10 0 0 1 -10,10m0,-22a12,12 0 0 0 -12,12a12,12 0 0 0 12,12a12,12 0 0 0 12,-12a12,12 0 0 0 -12,-12"}),s("path",{class:"q-radio__check",d:"M12,6a6,6 0 0 0 -6,6a6,6 0 0 0 6,6a6,6 0 0 0 6,-6a6,6 0 0 0 -6,-6"})]);var pt=q({name:"QRadio",props:{...F,...qe,...ze,modelValue:{required:!0},val:{required:!0},label:String,leftLabel:Boolean,checkedIcon:String,uncheckedIcon:String,color:String,keepColor:Boolean,dense:Boolean,disable:Boolean,tabindex:[String,Number]},emits:["update:modelValue"],setup(e,{slots:a,emit:t}){const{proxy:l}=Y(),r=J(e,l.$q),n=Ie(e,Re),i=z(null),{refocusTargetEl:d,refocusTarget:u}=Ve(e,i),f=c(()=>D(e.modelValue)===D(e.val)),o=c(()=>"q-radio cursor-pointer no-outline row inline no-wrap items-center"+(e.disable===!0?" disabled":"")+(r.value===!0?" q-radio--dark":"")+(e.dense===!0?" q-radio--dense":"")+(e.leftLabel===!0?" reverse":"")),y=c(()=>{const b=e.color!==void 0&&(e.keepColor===!0||f.value===!0)?` text-${e.color}`:"";return`q-radio__inner relative-position q-radio__inner--${f.value===!0?"truthy":"falsy"}${b}`}),g=c(()=>(f.value===!0?e.checkedIcon:e.uncheckedIcon)||null),S=c(()=>e.disable===!0?-1:e.tabindex||0),k=c(()=>{const b={type:"radio"};return e.name!==void 0&&Object.assign(b,{".checked":f.value===!0,"^checked":f.value===!0?"checked":void 0,name:e.name,value:e.val}),b}),m=De(k);function x(b){b!==void 0&&(O(b),u(b)),e.disable!==!0&&f.value!==!0&&t("update:modelValue",e.val,b)}function _(b){(b.keyCode===13||b.keyCode===32)&&O(b)}function C(b){(b.keyCode===13||b.keyCode===32)&&x(b)}return Object.assign(l,{set:x}),()=>{const b=g.value!==null?[s("div",{key:"icon",class:"q-radio__icon-container absolute-full flex flex-center no-wrap"},[s(H,{class:"q-radio__icon",name:g.value})])]:[gt];e.disable!==!0&&m(b,"unshift"," q-radio__native q-ma-none q-pa-none");const T=[s("div",{class:y.value,style:n.value,"aria-hidden":"true"},b)];d.value!==null&&T.push(d.value);const E=e.label!==void 0?me(a.default,[e.label]):M(a.default);return E!==void 0&&T.push(s("div",{class:"q-radio__label q-anchor--skip"},E)),s("div",{ref:i,class:o.value,tabindex:S.value,role:"radio","aria-label":e.label,"aria-checked":f.value===!0?"true":"false","aria-disabled":e.disable===!0?"true":void 0,onClick:x,onKeydown:_,onKeyup:C},T)}}});const Oe={...F,...qe,...ze,modelValue:{required:!0,default:null},val:{},trueValue:{default:!0},falseValue:{default:!1},indeterminateValue:{default:null},checkedIcon:String,uncheckedIcon:String,indeterminateIcon:String,toggleOrder:{type:String,validator:e=>e==="tf"||e==="ft"},toggleIndeterminate:Boolean,label:String,leftLabel:Boolean,color:String,keepColor:Boolean,dense:Boolean,disable:Boolean,tabindex:[String,Number]},He=["update:modelValue"];function Ye(e,a){const{props:t,slots:l,emit:r,proxy:n}=Y(),{$q:i}=n,d=J(t,i),u=z(null),{refocusTargetEl:f,refocusTarget:o}=Ve(t,u),y=Ie(t,Re),g=c(()=>t.val!==void 0&&Array.isArray(t.modelValue)),S=c(()=>{const v=D(t.val);return g.value===!0?t.modelValue.findIndex(h=>D(h)===v):-1}),k=c(()=>g.value===!0?S.value>-1:D(t.modelValue)===D(t.trueValue)),m=c(()=>g.value===!0?S.value===-1:D(t.modelValue)===D(t.falseValue)),x=c(()=>k.value===!1&&m.value===!1),_=c(()=>t.disable===!0?-1:t.tabindex||0),C=c(()=>`q-${e} cursor-pointer no-outline row inline no-wrap items-center`+(t.disable===!0?" disabled":"")+(d.value===!0?` q-${e}--dark`:"")+(t.dense===!0?` q-${e}--dense`:"")+(t.leftLabel===!0?" reverse":"")),b=c(()=>{const v=k.value===!0?"truthy":m.value===!0?"falsy":"indet",h=t.color!==void 0&&(t.keepColor===!0||(e==="toggle"?k.value===!0:m.value!==!0))?` text-${t.color}`:"";return`q-${e}__inner relative-position non-selectable q-${e}__inner--${v}${h}`}),T=c(()=>{const v={type:"checkbox"};return t.name!==void 0&&Object.assign(v,{".checked":k.value,"^checked":k.value===!0?"checked":void 0,name:t.name,value:g.value===!0?t.val:t.trueValue}),v}),E=De(T),G=c(()=>{const v={tabindex:_.value,role:e==="toggle"?"switch":"checkbox","aria-label":t.label,"aria-checked":x.value===!0?"mixed":k.value===!0?"true":"false"};return t.disable===!0&&(v["aria-disabled"]="true"),v});function B(v){v!==void 0&&(O(v),o(v)),t.disable!==!0&&r("update:modelValue",$(),v)}function $(){if(g.value===!0){if(k.value===!0){const v=t.modelValue.slice();return v.splice(S.value,1),v}return t.modelValue.concat([t.val])}if(k.value===!0){if(t.toggleOrder!=="ft"||t.toggleIndeterminate===!1)return t.falseValue}else if(m.value===!0){if(t.toggleOrder==="ft"||t.toggleIndeterminate===!1)return t.trueValue}else return t.toggleOrder!=="ft"?t.trueValue:t.falseValue;return t.indeterminateValue}function Q(v){(v.keyCode===13||v.keyCode===32)&&O(v)}function K(v){(v.keyCode===13||v.keyCode===32)&&B(v)}const U=a(k,x);return Object.assign(n,{toggle:B}),()=>{const v=U();t.disable!==!0&&E(v,"unshift",` q-${e}__native absolute q-ma-none q-pa-none`);const h=[s("div",{class:b.value,style:y.value,"aria-hidden":"true"},v)];f.value!==null&&h.push(f.value);const w=t.label!==void 0?me(l.default,[t.label]):M(l.default);return w!==void 0&&h.push(s("div",{class:`q-${e}__label q-anchor--skip`},w)),s("div",{ref:u,class:C.value,...G.value,onClick:B,onKeydown:Q,onKeyup:K},h)}}const vt=s("div",{key:"svg",class:"q-checkbox__bg absolute"},[s("svg",{class:"q-checkbox__svg fit absolute-full",viewBox:"0 0 24 24"},[s("path",{class:"q-checkbox__truthy",fill:"none",d:"M1.73,12.91 8.1,19.28 22.79,4.59"}),s("path",{class:"q-checkbox__indet",d:"M4,14H20V10H4"})])]);var bt=q({name:"QCheckbox",props:Oe,emits:He,setup(e){function a(t,l){const r=c(()=>(t.value===!0?e.checkedIcon:l.value===!0?e.indeterminateIcon:e.uncheckedIcon)||null);return()=>r.value!==null?[s("div",{key:"icon",class:"q-checkbox__icon-container absolute-full flex flex-center no-wrap"},[s(H,{class:"q-checkbox__icon",name:r.value})])]:[vt]}return Ye("checkbox",a)}}),yt=q({name:"QToggle",props:{...Oe,icon:String,iconColor:String},emits:He,setup(e){function a(t,l){const r=c(()=>(t.value===!0?e.checkedIcon:l.value===!0?e.indeterminateIcon:e.uncheckedIcon)||e.icon),n=c(()=>t.value===!0?e.iconColor:null);return()=>[s("div",{class:"q-toggle__track"}),s("div",{class:"q-toggle__thumb absolute flex flex-center no-wrap"},r.value!==void 0?[s(H,{name:r.value,color:n.value})]:void 0)]}return Ye("toggle",a)}});const $e={radio:pt,checkbox:bt,toggle:yt},wt=Object.keys($e);var xt=q({name:"QOptionGroup",props:{...F,modelValue:{required:!0},options:{type:Array,validator:e=>e.every(a=>"value"in a&&"label"in a)},name:String,type:{default:"radio",validator:e=>wt.includes(e)},color:String,keepColor:Boolean,dense:Boolean,size:String,leftLabel:Boolean,inline:Boolean,disable:Boolean},emits:["update:modelValue"],setup(e,{emit:a,slots:t}){const{proxy:{$q:l}}=Y(),r=Array.isArray(e.modelValue);e.type==="radio"?r===!0&&console.error("q-option-group: model should not be array"):r===!1&&console.error("q-option-group: model should be array in your case");const n=J(e,l),i=c(()=>$e[e.type]),d=c(()=>"q-option-group q-gutter-x-sm"+(e.inline===!0?" q-option-group--inline":"")),u=c(()=>{const o={role:"group"};return e.type==="radio"&&(o.role="radiogroup",e.disable===!0&&(o["aria-disabled"]="true")),o});function f(o){a("update:modelValue",o)}return()=>s("div",{class:d.value,...u.value},e.options.map((o,y)=>{const g=t["label-"+y]!==void 0?()=>t["label-"+y](o):t.label!==void 0?()=>t.label(o):void 0;return s("div",[s(i.value,{modelValue:e.modelValue,val:o.value,name:o.name===void 0?e.name:o.name,disable:e.disable||o.disable,label:g===void 0?o.label:null,leftLabel:o.leftLabel===void 0?e.leftLabel:o.leftLabel,color:o.color===void 0?e.color:o.color,checkedIcon:o.checkedIcon,uncheckedIcon:o.uncheckedIcon,dark:o.dark||n.value,size:o.size===void 0?e.size:o.size,dense:e.dense,keepColor:o.keepColor===void 0?e.keepColor:o.keepColor,"onUpdate:modelValue":f},g)])}))}});const fe={left:!0,right:!0,up:!0,down:!0,horizontal:!0,vertical:!0},kt=Object.keys(fe);fe.all=!0;function we(e){const a={};for(const t of kt)e[t]===!0&&(a[t]=!0);return Object.keys(a).length===0?fe:(a.horizontal===!0?a.left=a.right=!0:a.left===!0&&a.right===!0&&(a.horizontal=!0),a.vertical===!0?a.up=a.down=!0:a.up===!0&&a.down===!0&&(a.vertical=!0),a.horizontal===!0&&a.vertical===!0&&(a.all=!0),a)}const St=["INPUT","TEXTAREA"];function xe(e,a){return a.event===void 0&&e.target!==void 0&&e.target.draggable!==!0&&typeof a.handler=="function"&&St.includes(e.target.nodeName.toUpperCase())===!1&&(e.qClonedBy===void 0||e.qClonedBy.indexOf(a.uid)===-1)}function _t(e){const a=[.06,6,50];return typeof e=="string"&&e.length&&e.split(":").forEach((t,l)=>{const r=parseFloat(t);r&&(a[l]=r)}),a}var Ct=Je({name:"touch-swipe",beforeMount(e,{value:a,arg:t,modifiers:l}){if(l.mouse!==!0&&X.has.touch!==!0)return;const r=l.mouseCapture===!0?"Capture":"",n={handler:a,sensitivity:_t(t),direction:we(l),noop:tt,mouseStart(i){xe(i,n)&&at(i)&&(ne(n,"temp",[[document,"mousemove","move",`notPassive${r}`],[document,"mouseup","end","notPassiveCapture"]]),n.start(i,!0))},touchStart(i){if(xe(i,n)){const d=i.target;ne(n,"temp",[[d,"touchmove","move","notPassiveCapture"],[d,"touchcancel","end","notPassiveCapture"],[d,"touchend","end","notPassiveCapture"]]),n.start(i)}},start(i,d){X.is.firefox===!0&&re(e,!0);const u=be(i);n.event={x:u.left,y:u.top,time:Date.now(),mouse:d===!0,dir:!1}},move(i){if(n.event===void 0)return;if(n.event.dir!==!1){O(i);return}const d=Date.now()-n.event.time;if(d===0)return;const u=be(i),f=u.left-n.event.x,o=Math.abs(f),y=u.top-n.event.y,g=Math.abs(y);if(n.event.mouse!==!0){if(o<n.sensitivity[1]&&g<n.sensitivity[1]){n.end(i);return}}else if(window.getSelection().toString()!==""){n.end(i);return}else if(o<n.sensitivity[2]&&g<n.sensitivity[2])return;const S=o/d,k=g/d;n.direction.vertical===!0&&o<g&&o<100&&k>n.sensitivity[0]&&(n.event.dir=y<0?"up":"down"),n.direction.horizontal===!0&&o>g&&g<100&&S>n.sensitivity[0]&&(n.event.dir=f<0?"left":"right"),n.direction.up===!0&&o<g&&y<0&&o<100&&k>n.sensitivity[0]&&(n.event.dir="up"),n.direction.down===!0&&o<g&&y>0&&o<100&&k>n.sensitivity[0]&&(n.event.dir="down"),n.direction.left===!0&&o>g&&f<0&&g<100&&S>n.sensitivity[0]&&(n.event.dir="left"),n.direction.right===!0&&o>g&&f>0&&g<100&&S>n.sensitivity[0]&&(n.event.dir="right"),n.event.dir!==!1?(O(i),n.event.mouse===!0&&(document.body.classList.add("no-pointer-events--children"),document.body.classList.add("non-selectable"),Ue(),n.styleCleanup=m=>{n.styleCleanup=void 0,document.body.classList.remove("non-selectable");const x=()=>{document.body.classList.remove("no-pointer-events--children")};m===!0?setTimeout(x,50):x()}),n.handler({evt:i,touch:n.event.mouse!==!0,mouse:n.event.mouse,direction:n.event.dir,duration:d,distance:{x:o,y:g}})):n.end(i)},end(i){n.event!==void 0&&(se(n,"temp"),X.is.firefox===!0&&re(e,!1),n.styleCleanup!==void 0&&n.styleCleanup(!0),i!==void 0&&n.event.dir!==!1&&O(i),n.event=void 0)}};if(e.__qtouchswipe=n,l.mouse===!0){const i=l.mouseCapture===!0||l.mousecapture===!0?"Capture":"";ne(n,"main",[[e,"mousedown","mouseStart",`passive${i}`]])}X.has.touch===!0&&ne(n,"main",[[e,"touchstart","touchStart",`passive${l.capture===!0?"Capture":""}`],[e,"touchmove","noop","notPassiveCapture"]])},updated(e,a){const t=e.__qtouchswipe;t!==void 0&&(a.oldValue!==a.value&&(typeof a.value!="function"&&t.end(),t.handler=a.value),t.direction=we(a.modifiers))},beforeUnmount(e){const a=e.__qtouchswipe;a!==void 0&&(se(a,"main"),se(a,"temp"),X.is.firefox===!0&&re(e,!1),a.styleCleanup!==void 0&&a.styleCleanup(),delete e.__qtouchswipe)}});function qt(){const e=new Map;return{getCache:function(a,t){return e[a]===void 0?e[a]=t:e[a]},getCacheWithFn:function(a,t){return e[a]===void 0?e[a]=t():e[a]}}}const It={name:{required:!0},disable:Boolean},ke={setup(e,{slots:a}){return()=>s("div",{class:"q-panel scroll",role:"tabpanel"},M(a.default))}},Tt={modelValue:{required:!0},animated:Boolean,infinite:Boolean,swipeable:Boolean,vertical:Boolean,transitionPrev:String,transitionNext:String,transitionDuration:{type:[String,Number],default:300},keepAlive:Boolean,keepAliveInclude:[String,Array,RegExp],keepAliveExclude:[String,Array,RegExp],keepAliveMax:Number},Bt=["update:modelValue","beforeTransition","transition"];function Pt(){const{props:e,emit:a,proxy:t}=Y(),{getCacheWithFn:l}=qt();let r,n;const i=z(null),d=z(null);function u(h){const w=e.vertical===!0?"up":"left";B((t.$q.lang.rtl===!0?-1:1)*(h.direction===w?1:-1))}const f=c(()=>[[Ct,u,void 0,{horizontal:e.vertical!==!0,vertical:e.vertical,mouse:!0}]]),o=c(()=>e.transitionPrev||`slide-${e.vertical===!0?"down":"right"}`),y=c(()=>e.transitionNext||`slide-${e.vertical===!0?"up":"left"}`),g=c(()=>`--q-transition-duration: ${e.transitionDuration}ms`),S=c(()=>typeof e.modelValue=="string"||typeof e.modelValue=="number"?e.modelValue:String(e.modelValue)),k=c(()=>({include:e.keepAliveInclude,exclude:e.keepAliveExclude,max:e.keepAliveMax})),m=c(()=>e.keepAliveInclude!==void 0||e.keepAliveExclude!==void 0);ie(()=>e.modelValue,(h,w)=>{const P=b(h)===!0?T(h):-1;n!==!0&&G(P===-1?0:P<T(w)?-1:1),i.value!==P&&(i.value=P,a("beforeTransition",h,w),nt(()=>{a("transition",h,w)}))});function x(){B(1)}function _(){B(-1)}function C(h){a("update:modelValue",h)}function b(h){return h!=null&&h!==""}function T(h){return r.findIndex(w=>w.props.name===h&&w.props.disable!==""&&w.props.disable!==!0)}function E(){return r.filter(h=>h.props.disable!==""&&h.props.disable!==!0)}function G(h){const w=h!==0&&e.animated===!0&&i.value!==-1?"q-transition--"+(h===-1?o.value:y.value):null;d.value!==w&&(d.value=w)}function B(h,w=i.value){let P=w+h;for(;P>-1&&P<r.length;){const Z=r[P];if(Z!==void 0&&Z.props.disable!==""&&Z.props.disable!==!0){G(h),n=!0,a("update:modelValue",Z.props.name),setTimeout(()=>{n=!1});return}P+=h}e.infinite===!0&&r.length!==0&&w!==-1&&w!==r.length&&B(h,h===-1?r.length:-1)}function $(){const h=T(e.modelValue);return i.value!==h&&(i.value=h),!0}function Q(){const h=b(e.modelValue)===!0&&$()&&r[i.value];return e.keepAlive===!0?[s(it,k.value,[s(m.value===!0?l(S.value,()=>({...ke,name:S.value})):ke,{key:S.value,style:g.value},()=>h)])]:[s("div",{class:"q-panel scroll",style:g.value,key:S.value,role:"tabpanel"},[h])]}function K(){if(r.length!==0)return e.animated===!0?[s(Te,{name:d.value},Q)]:Q()}function U(h){return r=ht(M(h.default,[])).filter(w=>w.props!==null&&w.props.slot===void 0&&b(w.props.name)===!0),r.length}function v(){return r}return Object.assign(t,{next:x,previous:_,goTo:C}),{panelIndex:i,panelDirectives:f,updatePanelsList:U,updatePanelIndex:$,getPanelContent:K,getEnabledPanels:E,getPanels:v,isValidPanelName:b,keepAliveProps:k,needsUniqueKeepAliveWrapper:m,goToPanelByOffset:B,goToPanel:C,nextPanel:x,previousPanel:_}}var Lt=q({name:"QTabPanel",props:It,setup(e,{slots:a}){return()=>s("div",{class:"q-tab-panel",role:"tabpanel"},M(a.default))}}),At=q({name:"QTabPanels",props:{...Tt,...F},emits:Bt,setup(e,{slots:a}){const t=Y(),l=J(e,t.proxy.$q),{updatePanelsList:r,getPanelContent:n,panelDirectives:i}=Pt(),d=c(()=>"q-tab-panels q-panel-parent"+(l.value===!0?" q-tab-panels--dark q-dark":""));return()=>(r(a),Ge("div",{class:d.value},n(),"pan",e.swipeable,()=>i.value))}});class ge{}pe(ge,"Data",[{type:"Paper",title:"Searching for Galactic H ii Regions from the LAMOST Database Based on the Multihead WDCNN Model",publicationYear:2023,publicationMonth:8,authors:["Mengxin Wang","Jingjing Wu","Bin Jiang","Yanxia Zhang"],abstract:'A H ii region is a kind of emission nebula, and more definite samples of H ii regions can help study the formation and evolution of galaxies. Hence, a systematic search for H ii regions is necessary. The Large Sky Area Multi-Object Fiber Spectroscopic Telescope (LAMOST) conducts medium-resolution spectroscopic surveys and provides abundant valuable spectra for unique and rare celestial body research. Therefore, the medium-resolution spectra of LAMOST are an ideal data source for searching for Galactic H ii regions. This study uses the LAMOST spectra to expand the current spectral sample of Galactic H ii regions through machine learning. Inspired by deep convolutional neural networks with wide first-layer kernels (WDCNN), a new spectral-screening method, multihead WDCNN, is proposed and implemented. Infrared criteria are further used for the identification of Galactic H ii region candidates. Experimental results show that the multihead WDCNN model is superior to other machine-learning methods and it can effectively extract spectral features and identify H ii regions from the massive spectral database. In the end, among all candidates, 57 H ii regions are identified and known in SIMBAD, and four objects are identified as "to be confirmed" Galactic H ii region candidates. The known H ii regions and H ii region candidates can be retrieved from the LAMOST website.',doi:"10.3847/1538-4365/acd6f9",url:"https://doi.org/10.3847/1538-4365/acd6f9",publicationJournal:"The Astrophysical Journal Supplement Series",showImgUrl:"/publications/mengxinwang_2023_8.webp"},{type:"Paper",title:"Automatic detection of cataclysmic variables from SDSS images",publicationYear:2023,publicationMonth:6,authors:["Junfeng Huang","Meixia Qu","Bin Jiang","Yanxia Zhang"],abstract:"Investigating rare and new objects have always been an important direction in astronomy. Cataclysmic variables (CVs) are ideal and natural celestial bodies for studying the accretion process of semi-detached binaries with accretion processes. However, the sample size of CVs must increase because a lager gap exists between the observational and the theoretical expanding CVs. Astronomy has entered the big data era and can provide massive images containing CV candidates. CVs as a type of faint celestial objects, are highly challenging to be identified directly from images using automatic manners. Deep learning has rapidly developed in intelligent image processing and has been widely applied in some astronomical fields with excellent detection results. YOLOX, as the latest YOLO framework, is advantageous in detecting small and dark targets. This work proposes an improved YOLOX-based framework according to the characteristics of CVs and Sloan Digital Sky Survey (SDSS) photometric images to train and verify the model to realise CV detection. We use the Convolutional Block Attention Module to increase the number of output features with the feature extraction network and adjust the feature fusion network to obtain fused features. Accordingly, the loss function is modified. Experimental results demonstrate that the improved model produces satisfactory results, with average accuracy (mean average Precision at 0.5) of 92.0%, Precision of 92.9%, Recall of 94.3%, and  F1\u2212score of 93.6% on the test set. The proposed method can efficiently achieve the identification of CVs in test samples and search for CV candidates in unlabeled images. The image data vastly outnumber the spectra in the SDSS-released data. With supplementary follow-up observations or spectra, the proposed model can help astronomers in seeking and detecting CVs in a new manner to ensure that a more extensive CV catalog can be built. The proposed model may also be applied to the detection of other kinds of celestial objects.",doi:"10.1017/pasa.2023.34",url:"https://doi.org/10.1017/pasa.2023.34",publicationJournal:"Publications of the Astronomical Society of Australia",showImgUrl:"/publications/junfenghuang_2023_6.webp"},{type:"Paper",title:"Identification of Blue Horizontal Branch Stars with Multimodal Fusion",publicationYear:2023,publicationMonth:8,authors:["Jiaqi Wei","Bin Jiang","Yanxia Zhang"],abstract:"Blue Horizontal Branch stars (BHBs) are ideal tracers to probe the global structure of the milky Way (MW), and the increased size of the BHB star sample could be helpful to accurately calculate the MW's enclosed mass and kinematics. Large survey telescopes have produced an increasing number of astronomical images and spectra. However, traditional methods of identifying BHBs are limited in dealing with the large scale of astronomical data. A fast and efficient way of identifying BHBs can provide a more significant sample for further analysis and research. Therefore, in order to fully use the various data observed and further improve the identification accuracy of BHBs, we have innovatively proposed and implemented a Bi-level attention mechanism-based Transformer multimodal fusion model, called Bi-level Attention in the Transformer with Multimodality (BATMM). The model consists of a spectrum encoder, an image encoder, and a Transformer multimodal fusion module. The Transformer enables the effective fusion of data from two modalities, namely image and spectrum, by using the proposed Bi-level attention mechanism, including cross-attention and self-attention. As a result, the information from the different modalities complements each other, thus improving the accuracy of the identification of BHBs. The experimental results show that the F1 score of the proposed BATMM is 94.78%, which is 21.77% and 2.76% higher than the image and spectral unimodality, respectively. It is therefore demonstrated that higher identification accuracy of BHBs can be achieved by means of using data from multiple modalities and employing an efficient data fusion strategy.",doi:"10.1088/1538-3873/acea43",url:"https://doi.org/10.1088/1538-3873/acea43",publicationJournal:"Publications of the Astronomical Society of the Pacific",showImgUrl:"/publications/jiaqiwei_2023_8.webp"},{type:"Paper",title:"LoyalDE: Improving the performance of Graph Neural Networks with loyal node discovery and emphasis",publicationYear:2023,publicationMonth:1,authors:["Haotong Wei","Yinlin Zhu","Xunkai Li","Bin Jiang"],abstract:"Recent years have witnessed an increasing focus on graph-based semi-supervised learning with Graph Neural Networks (GNNs). Despite existing GNNs having achieved remarkable accuracy, research on the quality of graph supervision information has inadvertently been ignored. In fact, there are significant differences in the quality of supervision information provided by different labeled nodes, and treating supervision information with different qualities equally may lead to sub-optimal performance of GNNs. We refer to this as the graph supervision loyalty problem, which is a new perspective for improving the performance of GNNs. In this paper, we devise FT-Score to quantify node loyalty by considering both the local feature similarity and the local topology similarity, and nodes with higher loyalty are more likely to provide higher-quality supervision. Based on this, we propose LoyalDE (Loyal Node Discovery and Emphasis), a model-agnostic hot-plugging training strategy, which can discover potential nodes with high loyalty to expand the training set, and then emphasize nodes with high loyalty during model training to improve performance. Experiments demonstrate that the graph supervision loyalty problem will fail most existing GNNs. In contrast, LoyalDE brings about at most 9.1% performance improvement to vanilla GNNs and consistently outperforms several state-of-the-art training strategies for semi-supervised node classification.",doi:"10.1016/j.neunet.2023.05.023",url:"https://doi.org/10.1016/j.neunet.2023.05.023",publicationJournal:"Neural Networks",showImgUrl:"/publications/haotongwei_2023_1.webp"},{type:"Paper",title:"Deep Multimodal Networks for M-type Star Classification with Paired Spectrum and Photometric Image",publicationYear:2023,publicationMonth:5,authors:["Jialin Gao","Jianyu Chen","Jiaqi Wei","Bin Jiang","A-Li Luo"],abstract:"Traditional stellar classification methods include spectral and photometric classification separately. Although satisfactory results can be achieved, the accuracy could be improved. In this paper, we pioneer a novel approach to deeply fuse the spectra and photometric images of the sources in an advanced multimodal network to enhance the model's discriminatory ability. We use Transformer as the fusion module and apply a spectrum\u2013image contrastive loss function to enhance the consistency of the spectrum and photometric image of the same source in two different feature spaces. We perform M-type stellar subtype classification on two data sets with high and low signal-to-noise ratio (S/N) spectra and corresponding photometric images, and the F1-score achieves 95.65% and 90.84%, respectively. In our experiments, we prove that our model effectively utilizes the information from photometric images and is more accurate than advanced spectrum and photometric image classifiers. Our contributions can be summarized as follows: (1) We propose an innovative idea for stellar classification that allows the model to simultaneously consider information from spectra and photometric images. (2) We discover the challenge of fusing low-S/N spectra and photometric images in the Transformer and provide a solution. (3) The effectiveness of Transformer for spectral classification is discussed for the first time and will inspire more Transformer-based spectral classification models.",doi:"10.1088/1538-3873/acc7ca",url:"https://doi.org/10.1088/1538-3873/acc7ca",publicationJournal:"Publications of the Astronomical Society of the Pacific",showImgUrl:"/publications/jialingao_2023_5.webp"},{type:"Paper",title:"LS-GAN: Iterative Language-based Image Manipulation via Long and Short Term Consistency Reasoning",publicationYear:2022,publicationMonth:10,authors:["Gaoxiang Cong","Liang Li","Zhenhuan Liu","Yunbin Tu","Weijun Qin","Shenyuan Zhang","Chengang Yan","Wenyu Wang","Bin Jiang"],abstract:"Iterative language-based image manipulation aims to edit images step by step according to user's linguistic instructions. The existing methods mostly focus on aligning the attributes and appearance of new-added visual elements with current instruction. However, they fail to maintain consistency between instructions and images as iterative rounds increase. To address this issue, we propose a novel Long and Short term consistency reasoning Generative Adversarial Network (LS-GAN), which enhances the awareness of previous objects with current instruction and better maintains the consistency with the user's intent under the continuous iterations. Specifically, we first design a Context-aware Phrase Encoder (CPE) to learn the user's intention by extracting different phrase-level information about the instruction. Further, we introduce a Long and Short term Consistency Reasoning (LSCR) mechanism. The long-term reasoning improves the model on semantic understanding and positional reasoning, while short-term reasoning ensures the ability to construct visual scenes based on linguistic instructions. Extensive results show that LS-GAN improves the generation quality in terms of both object identity and position, and achieves the state-of-the-art performance on two public datasets.",doi:"10.1145/3503161.3548206",url:"https://doi.org/10.1145/3503161.3548206",publicationJournal:"Proceedings of the 29th ACM International Conference on Multimedia",showImgUrl:"/publications/gaoxiangcong_2022_10.webp"},{type:"Paper",title:"SEL-RefineMask: A Seal Segmentation and Recognition Neural Network with SEL-FPN",publicationYear:2022,publicationMonth:6,authors:["Zedong Dun","Jianyu Chen","Meixia Qu","Bin Jiang"],abstract:"Digging historical and cultural information from seals in ancient books is of great significance. However, ancient Chinese seal samples are scarce and carving methods are diverse, and traditional digital image processing methods based on greyscale have difficulty achieving superior segmentation and recognition performance. Recently, some deep learning algorithms have been proposed to address this problem; however, current neural networks are difficult to train owing to the lack of datasets. To solve the afore-mentioned problems, we proposed an SEL-RefineMask which combines selector of feature pyramid network (SEL-FPN) with RefineMask to segment and recognize seals. We designed an SEL-FPN to intelligently select a specific layer which represents different scales in the FPN and reduces the number of anchor frames. We performed experiments on some instance segmentation networks as the baseline method, and the top-1 segmentation result of 64.93% is 5.73% higher than that of humans. The top-1 result of the SEL-RefineMask network reached 67.96% which surpassed the baseline results. After segmentation, a vision transformer was used to recognize the segmentation output, and the accuracy reached 91%. Furthermore, a dataset of seals in ancient Chinese books (SACB) for segmentation and small seal font (SSF) for recognition were established which are publicly available on the website. The proposed method can be used to segment and recognize seals in ancient Chinese books.",doi:"10.3745/JIPS.02.0174",url:"https://doi.org/10.3745/JIPS.02.0174",publicationJournal:"Journal of Information Processing Systems",showImgUrl:"/publications/zedongdun_2022_6.webp"},{type:"Paper",title:"Automatic Detection and Classification of Radio Galaxy Images by Deep Learning",publicationYear:2022,publicationMonth:6,authors:["Zhen Zhang","Bin Jiang","Yanxia Zhang"],abstract:"Surveys conducted by radio astronomy observatories, such as SKA, MeerKAT, Very Large Array, and ASKAP, have generated massive astronomical images containing radio galaxies (RGs). This generation of massive RG images has imposed strict requirements on the detection and classification of RGs and makes manual classification and detection increasingly difficult, even impossible. Rapid classification and detection of images of different types of RGs help astronomers make full use of the observed astronomical image data for further processing and analysis. The classification of FRI and FRII is relatively easy, and there are more studies and literature on them at present, but FR0 and FRI are similar, so it is difficult to distinguish them. It poses a greater challenge to image processing. At present, deep learning has made breakthrough progress in the field of image analysis and processing and has preliminary applications in astronomical data processing. Compared with classification algorithms that can only classify galaxies, object detection algorithms that can locate and classify RGs simultaneously are preferred. In target detection algorithms, YOLOv5 has outstanding advantages in the classification and positioning of small targets. Therefore, we propose a deep-learning method based on an improved YOLOv5 object detection model that makes full use of multisource data, combining FIRST radio with SDSS optical image data, and realizes the automatic detection of FR0, FRI, and FRII RGs. The innovation of our work is that on the basis of the original YOLOv5 object detection model, we introduce the SE Net attention mechanism, increase the number of preset anchors, adjust the network structure of the feature pyramid, and modify the network structure, thereby allowing our model to demonstrate galaxy classification and position detection effects. Our improved model produces satisfactory results, as evidenced by experiments. Overall, the mean average precision (mAP@0.5) of our improved model on the test set reaches 89.4%, which can determine the position (R.A. and decl.) and automatically detect and classify FR0s, FRIs, and FRIIs. Our work contributes to astronomy because it allows astronomers to locate FR0, FRI, and FRII galaxies in a relatively short time and can be further combined with other astronomically generated data to study the properties of these galaxies. The target detection model can also help astronomers find FR0s, FRIs, and FRIIs in future surveys and build a large-scale star RG catalog. Moreover, our work is also useful for the detection of other types of galaxies.",doi:"10.1088/1538-3873/ac67b1",url:"https://doi.org/10.1088/1538-3873/ac67b1",publicationJournal:"Publications of the Astronomical Society of the Pacific",showImgUrl:"/publications/zhenzhang_2022_6.webp"},{type:"Paper",title:"A Scalable Deep Network for Graph Clustering via Personalized PageRank",publicationYear:2022,publicationMonth:5,authors:["Yulin Zhao","Xunkai Li","Yinlin Zhu","Jin Li","Shuo Wang","Bin Jiang"],abstract:"Recently, many models based on the combination of graph convolutional networks and deep learning have attracted extensive attention for their superior performance in graph clustering tasks. However, the existing models have the following limitations: (1) Existing models are limited by the calculation method of graph convolution, and their computational cost will increase exponentially as the graph scale grows. (2) Stacking too many convolutional layers causes the over-smoothing issue and neglects the local graph structure. (3) Expanding the range of the neighborhood and the model depth together is difficult due to the orthogonal relationship between them. Inspired by personalized pagerank and auto-encoder, we conduct the node-wise graph clustering task in the undirected simple graph as the research direction and propose a Scalable Deep Network (SDN) for graph clustering via personalized pagerank. Specifically, we utilize the combination of multi-layer perceptrons and linear propagation layer based on personalized pagerank as the backbone network (i.e., the Quasi-GNN module) and employ a DNN module for auto-encoder to learn different dimensions embeddings. After that, SDN combines the two embeddings correspondingly; then, it utilizes a dual self-supervised module to constrain the training of the embedding and clustering process. Our proposed Quasi-GNN module reduces the computational costs of traditional GNN models in a decoupled approach and solves the orthogonal relationship between the model depth and the neighborhood range. Meanwhile, it also alleviates the degraded clustering effect caused by the over-smoothing issue. We conducted experiments on five widely used graph datasets. The experimental results demonstrate that our model achieves state-of-the-art performance.",doi:"10.3390/app12115502",url:"https://doi.org/10.3390/app12115502",publicationJournal:"Applied Sciences",showImgUrl:"/publications/yulin_zhao_2022_5.webp"},{type:"Paper",title:"TribranchU-Net: A Size-Sensitive Network for Orbital Tumor Segmentation",publicationYear:2023,publicationMonth:4,authors:["Yuchen He","Wenyu Wang","Zhiming Cheng","Wei Qi","Zhigang Duan","Kai jin","Juan Ye","Shuai Wang"],abstract:"The shape and location of the orbital tumor are precious for tumor evaluation, diagnosis, and treatment, so it is essential to accurately segment the orbital tumor to assist clinicians in making reasonable decisions. Nowadays, encoder-decoder-based convolutional neural networks have been widely used in medical image segmentation tasks. However, the commonly used single tensor flow architecture cannot achieve satisfactory performance for segmentation with significant size variations. In this paper, we propose a size-sensitive deep network named TriBranchU-Net, which consists of a Small Branch with fewer down-sampling layers to enhance the feature learning of small regions and a Large Branch with atrous convolutional residual connections to enhance the feature learning of large regions. Then a multi-branch fusion module is designed to fuse the learned knowledge from different branches for the final segmentation. We built a large dataset for evaluation, including 602 CT images from 64 patients with orbital tumors. The experimental results show that our method can achieve superior performance in different sizes. Moreover, additional experimental results on the CVC-ClinicDB dataset further demonstrate that our method can better handle the size variation in segmentation.",doi:"10.1109/ISBI53787.2023.10230640",url:"https://doi.org/10.1109/ISBI53787.2023.10230640",publicationJournal:"2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI)",showImgUrl:"/publications/yuchen_he_2023_4.webp"},{type:"Paper",title:"AstroYOLO: A hybrid CNN\u2013Transformer deep-learning object-detection model for blue horizontal-branch stars",publicationYear:2023,publicationMonth:10,authors:["Yuchen He","Jingjing Wu","Wenyu Wang","Bin Jiang","Yanxia Zhang"],abstract:"Blue horizontal-branch stars (BHBs) are ideal tracers for studying the Milky Way (MW) due to their bright and nearly constant magnitude. However, an incomplete screen of BHBs from a survey would result in bias of estimation of the structure or mass of the MW. With surveys of large sky telescopes like the Sloan Digital Sky Survey (SDSS), it is possible to obtain a complete sample. Thus, detecting BHBs from massive photometric images quickly and effectually is necessary. The current acquisition methods of BHBs are mainly based on manual or semi-automatic modes. Therefore, novel approaches are required to replace manual or traditional machine-learning detection. The mainstream deep-learning-based object-detection methods are often vanilla convolutional neural networks whose ability to extract global features is limited by the receptive field of the convolution operator. Recently, a new Transformer-based method has benefited from the global receptive field advantage brought by the self-attention mechanism, exceeded the vanilla convolution model in many tasks, and achieved excellent results. Therefore, this paper proposes a hybrid convolution and Transformer model called AstroYOLO to take advantage of the convolution in local feature representation and Transformer\u2019s easier discovery of long-distance feature dependences. We conduct a comparative experiment on the 4799 SDSS DR16 photometric image dataset. The experimental results show that our model achieves 99.25% AP@50, 93.79% AP@75, and 64.45% AP@95 on the test dataset, outperforming the YOLOv3 and YOLOv4 object-detection models. In addition, we test on larger cutout images based on the same resolution. Our model can reach 99.02% AP@50, 92.00% AP@75, and 61.96% AP@95 respectively, still better than YOLOv3 and YOLOv4. These results also suggest that an appropriate size for cutout images is necessary for the performance and computation of object detection. Compared with the previous models, our model has achieved satisfactory object-detection results and can effectively improve the accuracy of BHB detection.",doi:"10.1093/pasj/psad071",url:"https://doi.org/10.1093/pasj/psad071",publicationJournal:"Publications of the Astronomical Society of Japan",showImgUrl:"/publications/yuchen_he_2023_10.webp"},{type:"Paper",title:"A cross-modal adversarial learning method for estimating photometric redshift of quasars",publicationYear:2023,publicationMonth:7,authors:["Chen Zhang","Wenyu Wang","Meixia Qu","Yanxia Zhang","Bin Jiang"],abstract:"Quasars play a crucial role in studying various important physical processes. We propose a cross modal deep learning method for estimating the photometric redshifts of quasars. Our model utilizes adversarial training to enable the conversion between photometric data features (magnitudes, colors, etc.) and photometric image features in five bands (u, g, r, i, z), in order to extract modality-invariant features. We used |\u2206z| = |(zphoto \u2212 zspec)/(1 + zspec)| as evaluation metric. The latest SOTA method, which implements cross-modal generation of simulated spectra from photometric data, has been chosen as the baseline. Firstly the proposed method was tested on the same SDSS DR17 dataset of 415,930 quasars(1 \u2264 zspec \u2264 5) as the baseline method. Compared to the baseline, the RMSE of our \u2206z decreased from 0.1235 to 0.1031. Further evaluation on a larger dataset of 465,292 quasars achieved a lower RMSE of \u2206z of 0.0861. This method also can be generalized to other tasks such as galaxy classification and redshift estimation.",url:"https://ml4astro.github.io/icml2023/assets/33.pdf",publicationJournal:"Proceedings of the 40th International Conference on Machine Learning",showImgUrl:"/publications/chenzhang_2023_7.webp"},{type:"Paper",title:"Dynamic Contrastive Learning with Pseudo-samples Intervention for Weakly Supervised Joint Video MR and HD",publicationYear:2023,publicationMonth:10,authors:["Shuhan Kong","Liang Li","Beichen Zhang","Wenyu Wang","Bin Jiang","Chenggang Yan","Changhao Xu"],abstract:"Joint video moment retrieval (MR) and highlight detection (HD) aims to find relevant video moments according to the query text. Existing methods are fully supervised based on manual annotation, and their coarse multi-modal information interactions easily lose details about video and text. In addition, some tasks introduce weakly supervised learning with random masks, while the single masking forces the model to focus on masked words and ignore multi-modal contextual information. In view of this, we attempt weakly supervised joint tasks (MR+HD) and propose Dynamic Contrastive Learning with Pseudo-Sample Intervention (CPI) for better multi-modal video comprehension. First, we design pseudo-samples over random masks for a more efficient contrastive learning manner. We introduce a proportional sampling strategy for pseudo-samples to ensure the semantic difference between the pseudo-samples and the query text. This balances the over-reliance from single random mask to global text semantics and makes the model learn multimodal context from each word fairly. Second, we design dynamic intervention contrastive loss to enhance the core feature-matching ability of the model dynamically. We add pseudo-sample intervention when negative proposals are close to positive proposals. This can help the model overcome the vision confusion phenomenon and achieve semantic similarity instead of word similarity. Extensive experiments demonstrate the effectiveness of CPI and the potential of weakly supervised joint tasks.",doi:"10.1145/3581783.3612384",url:"https://doi.org/10.1145/3581783.3612384",publicationJournal:"ACM Multimedia 2023",showImgUrl:"/publications/shuhankong_2023_10.webp"},{type:"Paper",title:"Effective hybrid graph and hypergraph convolution network for collaborative filtering",publicationYear:2022,publicationMonth:9,authors:["Xunkai Li","Ronghui Guo","jianwen Chen","Youpeng Hu","Meixia Qu","Bin Jiang"],abstract:"In recent years, graph convolution networks and hypergraph convolution networks have become a research hotspot in collaborative filtering (CF) because of their information extraction ability in dealing with the user-item interaction information. In particular, hypergraph can model high-order correlation of users and items to achieve better performance. However, the existing graph-based CF methods for mining interactive information remain incomplete and limit the expressiveness of the model. Moreover, they directly use low-order Chebyshev polynomials to fit the convolution kernel of graph and hypergraph without experimental proof or analysis, lacking interpretability. We propose an effective hybrid graph and hypergraph convolutional network (EHGCN) for CF to obtain a capable and interpretable framework. In EHGCN, the graph and the hypergraph are used to model the correlation among nodes in the interaction graph for multilevel learning. EHGCN also optimizes the information flow framework to match the improved convolution strategy of the graph and hypergraph we proposed. Extensive experiments on four real-world datasets show the considerable improvements of EHGCN over other state-of-the-art methods. Moreover, we analyze the graph and hypergraph convolution kernel in terms of the spectral domain to reveal the core of the graph-based CF, which has a heuristic effect on future work.",doi:"10.1007/s00521-022-07735-y",url:"https://doi.org/10.1007/s00521-022-07735-y",publicationJournal:"Neural Computing and Applications",showImgUrl:"/publications/xunkaili_2022_9.webp"},{type:"Paper",title:"Machine learning assisted design of high-entropy alloys with ultra-high microhardness and unexpected low density",publicationYear:2024,publicationMonth:1,authors:["Shunli Zhao","Bin Jiang","Kaikai Song","Xiaoming Liu","Wenyu Wang","Dekun Si","Jilei Zhang","Xiangyan Chen","Changshan Zhou","Pingping Liu","Dong Chen","Zequn Zhang","Parthiban Ramasamy","Junlei Tang","Wenquan Lv","Konda Gokuldoss Prashanth","Daniel \u015Eopu","J\xFCrgen Eckert"],abstract:"High-entropy alloys (HEAs) have attracted considerable attention for their exceptional microstructures and properties. Discovering new HEAs with desirable properties is crucial, but traditional design methods are laborious and time-consuming. Fortunately, the emerging Machine Learning (ML) offers an efficient solution. In this study, composition-microhardness data pairs from various alloy systems were collected and expanded using a Generative Adversarial Network (GAN). These data pairs were converted into empirical parameter-microhardness pairs. Then Active Learning (AL) was employed to screen the Al-Co-Cr-Cu-Fe-Ni system and identify the eXtreme Gradient Boosting (XGBoost) as the optimal ML master model. Millions of data training iterations employing the XGBoost sub-model and accuracy evaluations using the Expected Improvement (EI) algorithm establish the relationship between HEA compositions and microhardness. The proposed sub-model aligns well with experimental data, wherein four Al-rich compositions exhibit ultra-high microhardness (>740 HV, with a maximum of \u223C780.3 HV) and low density (<5.9 g/cm3) in the as-cast bulk state. The hardening increment originates from the precipitation of disordered BCC nanoparticles in the ordered AlCo-rich B2 matrix compared to the dilute B2 AlCo intermetallics. This lightweight, high-performance alloy shows potential for engineering applications as thin films or coatings.",doi:"10.1016/j.matdes.2024.112634",url:"https://doi.org/10.1016/j.matdes.2024.112634",publicationJournal:"Materials & Design",showImgUrl:"/publications/shunlizhao_2024_1.webp"}]);const Nt=["top","middle","bottom"];var Se=q({name:"QBadge",props:{color:String,textColor:String,floating:Boolean,transparent:Boolean,multiLine:Boolean,outline:Boolean,rounded:Boolean,label:[Number,String],align:{type:String,validator:e=>Nt.includes(e)}},setup(e,{slots:a}){const t=c(()=>e.align!==void 0?{verticalAlign:e.align}:null),l=c(()=>{const r=e.outline===!0&&e.color||e.textColor;return`q-badge flex inline items-center no-wrap q-badge--${e.multiLine===!0?"multi":"single"}-line`+(e.outline===!0?" q-badge--outline":e.color!==void 0?` bg-${e.color}`:"")+(r!==void 0?` text-${r}`:"")+(e.floating===!0?" q-badge--floating":"")+(e.rounded===!0?" q-badge--rounded":"")+(e.transparent===!0?" q-badge--transparent":"")});return()=>s("div",{class:l.value,style:t.value,role:"status","aria-label":e.label},me(a.default,e.label!==void 0?[e.label]:[]))}}),_e=q({name:"QItemLabel",props:{overline:Boolean,caption:Boolean,header:Boolean,lines:[Number,String]},setup(e,{slots:a}){const t=c(()=>parseInt(e.lines,10)),l=c(()=>"q-item__label"+(e.overline===!0?" q-item__label--overline text-overline":"")+(e.caption===!0?" q-item__label--caption text-caption":"")+(e.header===!0?" q-item__label--header":"")+(t.value===1?" ellipsis":"")),r=c(()=>e.lines!==void 0&&t.value>1?{overflow:"hidden",display:"-webkit-box","-webkit-box-orient":"vertical","-webkit-line-clamp":t.value}:null);return()=>s("div",{style:r.value,class:l.value},M(a.default))}}),Mt=q({name:"QSlideTransition",props:{appear:Boolean,duration:{type:Number,default:300}},emits:["show","hide"],setup(e,{slots:a,emit:t}){let l=!1,r,n,i=null,d=null,u,f;function o(){r&&r(),r=null,l=!1,i!==null&&(clearTimeout(i),i=null),d!==null&&(clearTimeout(d),d=null),n!==void 0&&n.removeEventListener("transitionend",u),u=null}function y(m,x,_){x!==void 0&&(m.style.height=`${x}px`),m.style.transition=`height ${e.duration}ms cubic-bezier(.25, .8, .50, 1)`,l=!0,r=_}function g(m,x){m.style.overflowY=null,m.style.height=null,m.style.transition=null,o(),x!==f&&t(x)}function S(m,x){let _=0;n=m,l===!0?(o(),_=m.offsetHeight===m.scrollHeight?0:void 0):(f="hide",m.style.overflowY="hidden"),y(m,_,x),i=setTimeout(()=>{i=null,m.style.height=`${m.scrollHeight}px`,u=C=>{d=null,(Object(C)!==C||C.target===m)&&g(m,"show")},m.addEventListener("transitionend",u),d=setTimeout(u,e.duration*1.1)},100)}function k(m,x){let _;n=m,l===!0?o():(f="show",m.style.overflowY="hidden",_=m.scrollHeight),y(m,_,x),i=setTimeout(()=>{i=null,m.style.height=0,u=C=>{d=null,(Object(C)!==C||C.target===m)&&g(m,"hide")},m.addEventListener("transitionend",u),d=setTimeout(u,e.duration*1.1)},100)}return Be(()=>{l===!0&&o()}),()=>s(Te,{css:!1,appear:e.appear,onEnter:S,onLeave:k},a.default)}});const Et={true:"inset",item:"item-inset","item-thumbnail":"item-thumbnail-inset"},de={xs:2,sm:4,md:8,lg:16,xl:24};var Ce=q({name:"QSeparator",props:{...F,spaced:[Boolean,String],inset:[Boolean,String],vertical:Boolean,color:String,size:String},setup(e){const a=Y(),t=J(e,a.proxy.$q),l=c(()=>e.vertical===!0?"vertical":"horizontal"),r=c(()=>` q-separator--${l.value}`),n=c(()=>e.inset!==!1?`${r.value}-${Et[e.inset]}`:""),i=c(()=>`q-separator${r.value}${n.value}`+(e.color!==void 0?` bg-${e.color}`:"")+(t.value===!0?" q-separator--dark":"")),d=c(()=>{const u={};if(e.size!==void 0&&(u[e.vertical===!0?"width":"height"]=e.size),e.spaced!==!1){const f=e.spaced===!0?`${de.md}px`:e.spaced in de?`${de[e.spaced]}px`:e.spaced,o=e.vertical===!0?["Left","Right"]:["Top","Bottom"];u[`margin${o[0]}`]=u[`margin${o[1]}`]=f}return u});return()=>s("hr",{class:i.value,style:d.value,"aria-orientation":l.value})}});const j=ot({}),Vt=Object.keys(Me);var zt=q({name:"QExpansionItem",props:{...Me,...Ze,...F,icon:String,label:String,labelLines:[Number,String],caption:String,captionLines:[Number,String],dense:Boolean,toggleAriaLabel:String,expandIcon:String,expandedIcon:String,expandIconClass:[Array,String,Object],duration:Number,headerInsetLevel:Number,contentInsetLevel:Number,expandSeparator:Boolean,defaultOpened:Boolean,hideExpandIcon:Boolean,expandIconToggle:Boolean,switchToggleSide:Boolean,denseToggle:Boolean,group:String,popup:Boolean,headerStyle:[Array,String,Object],headerClass:[Array,String,Object]},emits:[...Ke,"click","afterShow","afterHide"],setup(e,{slots:a,emit:t}){const{proxy:{$q:l}}=Y(),r=J(e,l),n=z(e.modelValue!==null?e.modelValue:e.defaultOpened),i=z(null),d=ve(),{show:u,hide:f,toggle:o}=Xe({showing:n});let y,g;const S=c(()=>`q-expansion-item q-item-type q-expansion-item--${n.value===!0?"expanded":"collapsed"} q-expansion-item--${e.popup===!0?"popup":"standard"}`),k=c(()=>{if(e.contentInsetLevel===void 0)return null;const p=l.lang.rtl===!0?"Right":"Left";return{["padding"+p]:e.contentInsetLevel*56+"px"}}),m=c(()=>e.disable!==!0&&(e.href!==void 0||e.to!==void 0&&e.to!==null&&e.to!=="")),x=c(()=>{const p={};return Vt.forEach(V=>{p[V]=e[V]}),p}),_=c(()=>m.value===!0||e.expandIconToggle!==!0),C=c(()=>e.expandedIcon!==void 0&&n.value===!0?e.expandedIcon:e.expandIcon||l.iconSet.expansionItem[e.denseToggle===!0?"denseIcon":"icon"]),b=c(()=>e.disable!==!0&&(m.value===!0||e.expandIconToggle===!0)),T=c(()=>({expanded:n.value===!0,detailsId:e.targetUid,toggle:o,show:u,hide:f})),E=c(()=>{const p=e.toggleAriaLabel!==void 0?e.toggleAriaLabel:l.lang.label[n.value===!0?"collapse":"expand"](e.label);return{role:"button","aria-expanded":n.value===!0?"true":"false","aria-controls":d,"aria-label":p}});ie(()=>e.group,p=>{g!==void 0&&g(),p!==void 0&&U()});function G(p){m.value!==!0&&o(p),t("click",p)}function B(p){p.keyCode===13&&$(p,!0)}function $(p,V){V!==!0&&i.value!==null&&i.value.focus(),o(p),O(p)}function Q(){t("afterShow")}function K(){t("afterHide")}function U(){y===void 0&&(y=ve()),n.value===!0&&(j[e.group]=y);const p=ie(n,ae=>{ae===!0?j[e.group]=y:j[e.group]===y&&delete j[e.group]}),V=ie(()=>j[e.group],(ae,je)=>{je===y&&ae!==void 0&&ae!==y&&f()});g=()=>{p(),V(),j[e.group]===y&&delete j[e.group],g=void 0}}function v(){const p={class:[`q-focusable relative-position cursor-pointer${e.denseToggle===!0&&e.switchToggleSide===!0?" items-end":""}`,e.expandIconClass],side:e.switchToggleSide!==!0,avatar:e.switchToggleSide},V=[s(H,{class:"q-expansion-item__toggle-icon"+(e.expandedIcon===void 0&&n.value===!0?" q-expansion-item__toggle-icon--rotated":""),name:C.value})];return b.value===!0&&(Object.assign(p,{tabindex:0,...E.value,onClick:$,onKeyup:B}),V.unshift(s("div",{ref:i,class:"q-expansion-item__toggle-focus q-icon q-focus-helper q-focus-helper--rounded",tabindex:-1}))),s(oe,p,()=>V)}function h(){let p;return a.header!==void 0?p=[].concat(a.header(T.value)):(p=[s(oe,()=>[s(_e,{lines:e.labelLines},()=>e.label||""),e.caption?s(_e,{lines:e.captionLines,caption:!0},()=>e.caption):null])],e.icon&&p[e.switchToggleSide===!0?"push":"unshift"](s(oe,{side:e.switchToggleSide===!0,avatar:e.switchToggleSide!==!0},()=>s(H,{name:e.icon})))),e.disable!==!0&&e.hideExpandIcon!==!0&&p[e.switchToggleSide===!0?"unshift":"push"](v()),p}function w(){const p={ref:"item",style:e.headerStyle,class:e.headerClass,dark:r.value,disable:e.disable,dense:e.dense,insetLevel:e.headerInsetLevel};return _.value===!0&&(p.clickable=!0,p.onClick=G,Object.assign(p,m.value===!0?x.value:E.value)),s(et,p,h)}function P(){return rt(s("div",{key:"e-content",class:"q-expansion-item__content relative-position",style:k.value,id:d},M(a.default)),[[st,n.value]])}function Z(){const p=[w(),s(Mt,{duration:e.duration,onShow:Q,onHide:K},P)];return e.expandSeparator===!0&&p.push(s(Ce,{class:"q-expansion-item__border q-expansion-item__border--top absolute-top",dark:r.value}),s(Ce,{class:"q-expansion-item__border q-expansion-item__border--bottom absolute-bottom",dark:r.value})),p}return e.group!==void 0&&U(),Be(()=>{g!==void 0&&g()}),()=>s("div",{class:S.value},[s("div",{class:"q-expansion-item__container relative-position"},Z())])}}),Dt=q({name:"QTimelineEntry",props:{heading:Boolean,tag:{type:String,default:"h3"},side:{type:String,default:"right",validator:e=>["left","right"].includes(e)},icon:String,avatar:String,color:String,title:String,subtitle:String,body:String},setup(e,{slots:a}){const t=lt(Pe,le);if(t===le)return console.error("QTimelineEntry needs to be child of QTimeline"),le;const l=c(()=>`q-timeline__entry q-timeline__entry--${e.side}`+(e.icon!==void 0||e.avatar!==void 0?" q-timeline__entry--icon":"")),r=c(()=>`q-timeline__dot text-${e.color||t.color}`),n=c(()=>t.layout==="comfortable"&&t.side==="left");return()=>{const i=Qe(a.default,[]);if(e.body!==void 0&&i.unshift(e.body),e.heading===!0){const f=[s("div"),s("div"),s(e.tag,{class:"q-timeline__heading-title"},i)];return s("div",{class:"q-timeline__heading"},n.value===!0?f.reverse():f)}let d;e.icon!==void 0?d=[s(H,{class:"row items-center justify-center",name:e.icon})]:e.avatar!==void 0&&(d=[s("img",{class:"q-timeline__dot-img",src:e.avatar})]);const u=[s("div",{class:"q-timeline__subtitle"},[s("span",{},M(a.subtitle,[e.subtitle]))]),s("div",{class:r.value},d),s("div",{class:"q-timeline__content"},[s("h6",{class:"q-timeline__title"},M(a.title,[e.title]))].concat(i))];return s("li",{class:l.value},n.value===!0?u.reverse():u)}}}),Rt=q({name:"QTimeline",props:{...F,color:{type:String,default:"primary"},side:{type:String,default:"right",validator:e=>["left","right"].includes(e)},layout:{type:String,default:"dense",validator:e=>["dense","comfortable","loose"].includes(e)}},setup(e,{slots:a}){const t=Y(),l=J(e,t.proxy.$q);ut(Pe,e);const r=c(()=>`q-timeline q-timeline--${e.layout} q-timeline--${e.layout}--${e.side}`+(l.value===!0?" q-timeline--dark":""));return()=>s("ul",{class:r.value},M(a.default))}});const Ot={class:"publication-page-panel-wrap bg-primary column flex-center full-width"},Ht={class:"row justify-start items-center"},Yt={class:"row justify-between items-center q-mr-md"},$t={class:"text-h6 text-bold"},jt={key:0,class:"col-4 q-pr-md"},Wt={class:"text-subtitle1 text-bold"},Ft={class:"text-subtitle2"},Jt={class:"abstract-p text-body1 q-my-md",style:{"text-align":"justify"}},Gt={class:"external-link-btns row flex-center"},Qt=Le({__name:"PublicationPagePanel",props:{currentYear:{type:Number,required:!0}},setup(e){const a=e,t=z(),l=Ee(),r=l.platform.is.mobile;function n(){t.value=ge.Data.filter(u=>u.publicationYear===a.currentYear).sort((u,f)=>u.publicationYear===f.publicationYear?f.publicationMonth-u.publicationMonth:f.publicationYear-u.publicationYear)}function i(u){const f=new Date;return f.setMonth(u-1),f.toLocaleString(l.lang.getLocale(),{month:"long"})}function d(u){return u.authors.length===1?u.authors[0]:u.authors.length===2?u.authors.join(" and "):u.authors.slice(0,-1).join(", ")+" and "+u.authors.slice(-1)}return n(),(u,f)=>(R(),te("div",Ot,[A(Rt,{class:"full-width",color:"accent"},{default:L(()=>[(R(!0),te(Ae,null,Ne(t.value,o=>(R(),ue(Dt,{key:o.doi},{subtitle:L(()=>[I("div",Ht,[ee(N(`${i(o.publicationMonth)}, ${o.publicationYear}`)+" ",1),A(Se,{class:"q-ml-md",color:"accent",outline:""},{default:L(()=>[ee(N(o.type),1)]),_:2},1024)])]),default:L(()=>[A(zt,{class:"full-width","default-opened":!0,"hide-expand-icon":W(r)},{header:L(()=>[I("div",Yt,[I("span",$t,N(o.title),1)])]),default:L(()=>[I("div",{class:he([W(r)?"":"q-pa-md","publication-info-card full-width row justify-center items-center"])},[o.showImgUrl&&!W(r)?(R(),te("div",jt,[A(mt,{class:"page-img shadow-2",src:o.showImgUrl,style:{height:"15rem","border-radius":"19px"},ratio:1,fit:"cover"},null,8,["src"])])):ce("",!0),I("div",{class:he([W(r)?"col-12":"col-8","column justify-start items-start"])},[o.doi?(R(),ue(Se,{key:0,color:"accent",class:"cursor-pointer",onClick:y=>W(ft)(`https://doi.org/${o.doi}`)},{default:L(()=>[ee(" doi: "+N(o.doi),1)]),_:2},1032,["onClick"])):ce("",!0),I("span",Wt,N(o.publicationJournal),1),I("span",Ft,N(d(o)),1),I("p",Jt,N(o.abstract),1),I("div",Gt,[o.codeUrl?(R(),ue(ye,{key:0,class:"q-mr-md",href:o.codeUrl,target:"_blank",outline:"","no-caps":""},{default:L(()=>[A(H,{name:"fa-brands fa-github",class:"q-mr-sm",size:"xs"}),ee(" "+N(u.$t("publicationPageSourceCode")),1)]),_:2},1032,["href"])):ce("",!0),A(ye,{href:o.url,target:"_blank",outline:"","no-caps":""},{default:L(()=>[A(H,{name:"fas fa-external-link-alt",class:"q-mr-sm",size:"xs"}),ee(" "+N(u.$t("publicationPageReadMoreBtn")),1)]),_:2},1032,["href"])])],2)],2)]),_:2},1032,["hide-expand-icon"])]),_:2},1024))),128))]),_:1})]))}});var Ut=ct(Qt,[["__scopeId","data-v-263bbfec"]]);const Zt={class:"publication-page-title"},Kt={class:"publication-page-tabs q-mt-md"},sa=Le({__name:"IndexPagePublications",setup(e){const t=Ee().platform.is.mobile,l=[],r=z(new Date().getFullYear());function n(){ge.Data.forEach(i=>{l.some(d=>d.value===i.publicationYear)||l.push({label:i.publicationYear.toString(),value:i.publicationYear})}),l.sort((i,d)=>d.value-i.value),r.value=l[0].value}return n(),(i,d)=>(R(),te("div",{class:he(["index-page-publications-wrap bg-transparent column flex-center full-width",W(t)?"q-pa-md":"q-pa-xl"])},[I("div",Zt,[I("span",{class:"text-accent text-bold",style:dt({fontSize:W(t)?"5vw":"2vw"})},N(i.$t("publicationPageTitle")),5)]),I("div",Kt,[A(xt,{color:"accent",modelValue:r.value,"onUpdate:modelValue":d[0]||(d[0]=u=>r.value=u),options:l,inline:""},null,8,["modelValue"])]),A(At,{modelValue:r.value,"onUpdate:modelValue":d[1]||(d[1]=u=>r.value=u),class:"bg-primary full-width",animated:""},{default:L(()=>[(R(),te(Ae,null,Ne(l,u=>A(Lt,{class:"full-width",key:u.value,name:u.value},{default:L(()=>[A(Ut,{"current-year":u.value},null,8,["current-year"])]),_:2},1032,["name"])),64))]),_:1},8,["modelValue"])],2))}});export{sa as default};
